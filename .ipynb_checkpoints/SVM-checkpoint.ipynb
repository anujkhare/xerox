{
 "metadata": {
  "name": "",
  "signature": "sha256:804183b2463dca46e1dbdff71feb96265a3b1ff576df1488d80c4c1038b2509d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%vimception"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Line magic function `%vimception` not found.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import pandas\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "df_age = pd.read_csv('data_train/id_age_train.csv')\n",
      "df_labels = pd.read_csv('data_train/id_label_train.csv')\n",
      "df_vitals = pd.read_csv('data_train/id_time_vitals_train.csv')\n",
      "df_labs = pd.read_csv('data_train/id_time_labs_train.csv')"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Impute with mean\n",
      "df_vitals_mean = df_vitals.copy()\n",
      "for i in xrange(1, 7):\n",
      "    col = 'V' + str(i)\n",
      "    index = df_vitals_mean.ix[:, col].isnull()\n",
      "    df_vitals_mean.ix[index, col] = df_vitals_mean.ix[index^1, col].mean()\n",
      "df_labs_mean = df_labs.copy()\n",
      "for i in xrange(1, 26):\n",
      "    col = 'L' + str(i)\n",
      "    index = df_labs_mean.ix[:, col].isnull()\n",
      "    df_labs_mean.ix[index, col] = df_labs_mean.ix[index^1, col].mean()\n",
      "# Combined values imputed with mean\n",
      "patient_combined_mean = [np.append(np.array(df_vitals_mean.ix[df_vitals_mean['ID'] == ID, 2:8]),\n",
      "                                   np.array(df_labs_mean.ix[df_labs_mean['ID'] == ID, 2:27]), axis=1)\n",
      "                         for ID in df_labels['ID'].unique()                        \n",
      "                         ]\n",
      "\n",
      "labels_combined = list(df_labels.ix[:, 'LABEL'])\n",
      "# Windows!\n",
      "window_size = 10\n",
      "stride = 1\n",
      "n_features = 31\n",
      "n_stats = 4\n",
      "data = patient_combined_mean\n",
      "labels = []\n",
      "window_stats = []\n",
      "\n",
      "for i in xrange(len(data)):\n",
      "    ts = data[i]\n",
      "    n_windows = ts.shape[0] - window_size + 1 # NO STRIDE!!\n",
      "    \n",
      "    if(n_windows <= 0):\n",
      "        #window_stats.append(None)    # We want to keep 1-1 mapping with patients..\n",
      "        continue\n",
      "    \n",
      "    l = np.zeros((n_windows, n_stats * n_features))\n",
      "    \n",
      "    for j in xrange(0, n_windows, stride):   # NO STRIDE!\n",
      "        window = ts[j : window_size+j, :] \n",
      "        l[j, :n_features] = np.nanmean(window, axis=0)\n",
      "        l[j, n_features:2*n_features] = np.nanvar(window, axis=0)\n",
      "        l[j, 2*n_features:3*n_features] = np.nanmin(window, axis=0)\n",
      "        l[j, 3*n_features:4*n_features] = np.nanmax(window, axis=0)\n",
      "    \n",
      "    window_stats.append(l)\n",
      "    labels.append(labels_combined[i])\n",
      "\n",
      "# SET teh global variable\n",
      "window_stats_patient = window_stats\n",
      "labels = np.array(labels)\n",
      "print len(window_stats_patient)\n",
      "print len(labels)\n",
      "### Save the data\n",
      "np.save('/root/code/xerox/windows/windows_combined', window_stats_patient)\n",
      "np.save('/root/code/xerox/windows/labels_combined', labels)\n",
      "### No. of patients with NO WINDOW\n",
      "print sum([window_stats_patient_dead[i] == None for i in xrange(n_dead)])\n",
      "print sum([window_stats_patient_living[i] == None for i in xrange(n_living)])\n",
      "print len(window_stats_patient_dead)\n",
      "print len(window_stats_patient_living)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_combined = np.load('/root/code/xerox/windows/windows_combined.npy')\n",
      "labels_combined = np.load('/root/code/xerox/windows/labels_combined.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(labels_combined)\n",
      "print len(data_combined)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3546\n",
        "3546\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Confusion!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def get_ss(confusion):\n",
      "    tn = confusion[0, 0]  # label=0, pred=0\n",
      "    fp = confusion[0, 1]  # label=0, pred=1\n",
      "    \n",
      "    fn = confusion[1, 0]  # label=1, pred=0\n",
      "    tp = confusion[1, 1]  # label=1, pred=1\n",
      "    \n",
      "    sens = (1.0*tp/(tp+fn))\n",
      "    spec = (1.0*tn/(tn+fp))\n",
      "    \n",
      "    print confusion\n",
      "    print\n",
      "    #print tp, fp, fn, tn, sens, spec\n",
      "    print 'Sensitiviy: ', sens, '\\nSpecificity: ', spec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# RANDOM FOREST"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = np.arange(0, len(labels_combined))\n",
      "np.random.shuffle(ind)\n",
      "N = len(ind)\n",
      "split = 0.6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N_train = int(split * N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind_train = ind[:N_train]\n",
      "ind_valid = ind[N_train:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Divide into sets and then stack to get dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generte the data matrix for stacked data\n",
      "data_train = np.vstack(data_combined[ind_train])\n",
      "data_valid = np.vstack(data_combined[ind_valid])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generate the labels for the stacked data\n",
      "labels_train = np.vstack([np.zeros((data_combined[i].shape[0], 1)) + labels_combined[i] for i in ind_train])\n",
      "labels_valid = np.vstack([np.zeros((data_combined[i].shape[0], 1)) + labels_combined[i] for i in ind_valid])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_train_dead = int(sum(labels_train)[0])\n",
      "n_train_living = int(sum(1 - labels_train)[0])\n",
      "n_train_combined = n_train_living + n_train_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_valid_dead = int(sum(labels_valid)[0])\n",
      "n_valid_living = int(sum(1 - labels_valid)[0])\n",
      "n_valid_combined = n_valid_living + n_valid_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print n_train_dead, n_train_living, labels_train.shape[0]\n",
      "print n_valid_dead, n_valid_living, labels_valid.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "43704 312142 355846\n",
        "27482 212904 240386\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate list of living and dead patients in train and validation dtta"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patient_train_living = [data_combined[i] for i in ind_train if labels_combined[i] ==0]\n",
      "patient_train_dead = [data_combined[i] for i in ind_train if labels_combined[i] ==1]\n",
      "patient_valid_living = [data_combined[i] for i in ind_valid if labels_combined[i] ==0]\n",
      "patient_valid_dead = [data_combined[i] for i in ind_valid if labels_combined[i] ==1]\n",
      "print len(patient_valid_living), len(patient_valid_dead)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1330 89\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_train_living = np.vstack(patient_train_living)\n",
      "data_train_dead = np.vstack(patient_train_dead)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data_train_living.shape, data_train_dead.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(312142, 124) (43704, 124)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Split training set into equal dead and living parts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_train_living/ n_train_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "7"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_split = int (n_train_living / (n_train_living / n_train_dead))\n",
      "n_train_splits = int (n_train_living / n_train_dead)\n",
      "print n_train_splits, train_split, n_train_splits * train_split, n_train_combined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 44591 312137 355846\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forests = [svm.NuSVC(kernel='poly', degree=2, gamma=10**(-1.7), probability=True)] * n_train_splits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#forests = [RandomForestClassifier(n_estimators=40, n_jobs=-1, max_depth=40)] * n_train_splits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for forest in forests:\n",
      "    importances = forest.feature_importances_\n",
      "\n",
      "    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
      "    indices = np.argsort(importances)[::-1]\n",
      "    # Print the feature ranking\n",
      "    print(\"Feature ranking:\")\n",
      "\n",
      "    for f in range(124):\n",
      "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
      "\n",
      "    # Plot the feature importances of the forest\n",
      "    plt.figure()\n",
      "    plt.title(\"Feature importances\")\n",
      "    plt.bar(range(124), importances[indices],\n",
      "           color=\"r\", yerr=std[indices], align=\"center\")\n",
      "    plt.xticks(range(124), indices)\n",
      "    plt.xlim([-1, 125])\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Feature ranking:\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n",
        "Feature ranking:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n",
        "Feature ranking:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n",
        "Feature ranking:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n",
        "Feature ranking:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n",
        "Feature ranking:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n",
        "Feature ranking:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1. feature 3 (0.046318)\n",
        "2. feature 2 (0.041879)\n",
        "3. feature 65 (0.035780)\n",
        "4. feature 0 (0.032836)\n",
        "5. feature 93 (0.032344)\n",
        "6. feature 95 (0.031353)\n",
        "7. feature 64 (0.031060)\n",
        "8. feature 1 (0.030017)\n",
        "9. feature 4 (0.029784)\n",
        "10. feature 62 (0.026957)\n",
        "11. feature 34 (0.026738)\n",
        "12. feature 36 (0.026177)\n",
        "13. feature 33 (0.025294)\n",
        "14. feature 94 (0.024770)\n",
        "15. feature 96 (0.024459)\n",
        "16. feature 5 (0.024248)\n",
        "17. feature 63 (0.023691)\n",
        "18. feature 35 (0.022138)\n",
        "19. feature 32 (0.021914)\n",
        "20. feature 31 (0.020475)\n",
        "21. feature 85 (0.020378)\n",
        "22. feature 23 (0.019997)\n",
        "23. feature 98 (0.019377)\n",
        "24. feature 66 (0.018701)\n",
        "25. feature 54 (0.018300)\n",
        "26. feature 67 (0.018253)\n",
        "27. feature 97 (0.012014)\n",
        "28. feature 74 (0.011554)\n",
        "29. feature 13 (0.011514)\n",
        "30. feature 116 (0.010504)\n",
        "31. feature 12 (0.010424)\n",
        "32. feature 75 (0.009319)\n",
        "33. feature 9 (0.008248)\n",
        "34. feature 11 (0.007518)\n",
        "35. feature 40 (0.007301)\n",
        "36. feature 16 (0.006783)\n",
        "37. feature 105 (0.006409)\n",
        "38. feature 10 (0.006309)\n",
        "39. feature 41 (0.006267)\n",
        "40. feature 26 (0.006075)\n",
        "41. feature 43 (0.005642)\n",
        "42. feature 102 (0.005426)\n",
        "43. feature 46 (0.005141)\n",
        "44. feature 44 (0.005141)\n",
        "45. feature 17 (0.005075)\n",
        "46. feature 15 (0.005034)\n",
        "47. feature 73 (0.004797)\n",
        "48. feature 47 (0.004711)\n",
        "49. feature 88 (0.004689)\n",
        "50. feature 14 (0.004648)\n",
        "51. feature 76 (0.004507)\n",
        "52. feature 78 (0.004160)\n",
        "53. feature 42 (0.003916)\n",
        "54. feature 71 (0.003757)\n",
        "55. feature 25 (0.003730)\n",
        "56. feature 45 (0.003619)\n",
        "57. feature 38 (0.003596)\n",
        "58. feature 72 (0.003579)\n",
        "59. feature 57 (0.003564)\n",
        "60. feature 69 (0.003559)\n",
        "61. feature 110 (0.003542)\n",
        "62. feature 7 (0.003508)\n",
        "63. feature 30 (0.003494)\n",
        "64. feature 106 (0.003480)\n",
        "65. feature 103 (0.003419)\n",
        "66. feature 77 (0.003408)\n",
        "67. feature 27 (0.003382)\n",
        "68. feature 48 (0.003155)\n",
        "69. feature 39 (0.003116)\n",
        "70. feature 61 (0.003018)\n",
        "71. feature 109 (0.002986)\n",
        "72. feature 8 (0.002898)\n",
        "73. feature 37 (0.002759)\n",
        "74. feature 6 (0.002756)\n",
        "75. feature 56 (0.002657)\n",
        "76. feature 20 (0.002604)\n",
        "77. feature 89 (0.002436)\n",
        "78. feature 70 (0.002421)\n",
        "79. feature 51 (0.002421)\n",
        "80. feature 108 (0.002323)\n",
        "81. feature 123 (0.002284)\n",
        "82. feature 87 (0.002262)\n",
        "83. feature 99 (0.002168)\n",
        "84. feature 92 (0.002040)\n",
        "85. feature 79 (0.001909)\n",
        "86. feature 119 (0.001832)\n",
        "87. feature 120 (0.001793)\n",
        "88. feature 21 (0.001756)\n",
        "89. feature 104 (0.001744)\n",
        "90. feature 58 (0.001744)\n",
        "91. feature 100 (0.001708)\n",
        "92. feature 83 (0.001648)\n",
        "93. feature 28 (0.001562)\n",
        "94. feature 118 (0.001549)\n",
        "95. feature 107 (0.001480)\n",
        "96. feature 52 (0.001478)\n",
        "97. feature 68 (0.001309)\n",
        "98. feature 59 (0.001257)\n",
        "99. feature 113 (0.001182)\n",
        "100. feature 82 (0.001182)\n",
        "101. feature 101 (0.001101)\n",
        "102. feature 90 (0.001031)\n",
        "103. feature 121 (0.000637)\n",
        "104. feature 19 (0.000284)\n",
        "105. feature 81 (0.000250)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 24 (0.000203)\n",
        "108. feature 55 (0.000150)\n",
        "109. feature 114 (0.000133)\n",
        "110. feature 86 (0.000112)\n",
        "111. feature 29 (0.000084)\n",
        "112. feature 117 (0.000082)\n",
        "113. feature 112 (0.000064)\n",
        "114. feature 60 (0.000054)\n",
        "115. feature 91 (0.000034)\n",
        "116. feature 122 (0.000032)\n",
        "117. feature 18 (0.000017)\n",
        "118. feature 49 (0.000015)\n",
        "119. feature 22 (0.000014)\n",
        "120. feature 111 (0.000008)\n",
        "121. feature 115 (0.000007)\n",
        "122. feature 84 (0.000004)\n",
        "123. feature 80 (0.000003)\n",
        "124. feature 53 (0.000001)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "array([  3,   2,  65,   0,  93,  95,  64,   1,   4,  62,  34,  36,  33,\n",
        "        94,  96,   5,  63,  35,  32,  31,  85,  23,  98,  66,  54,  67,\n",
        "        97,  74,  13, 116,  12,  75,   9,  11,  40,  16, 105,  10,  41,\n",
        "        26,  43, 102,  46,  44,  17,  15,  73,  47,  88,  14,  76,  78,\n",
        "        42,  71,  25,  45,  38,  72,  57,  69, 110,   7,  30, 106, 103,\n",
        "        77,  27,  48,  39,  61, 109,   8,  37,   6,  56,  20,  89,  70,\n",
        "        51, 108, 123,  87,  99,  92,  79, 119, 120,  21, 104,  58, 100,\n",
        "        83,  28, 118, 107,  52,  68,  59, 113,  82, 101,  90, 121,  19,\n",
        "        81,  50,  24,  55, 114,  86,  29, 117, 112,  60,  91, 122,  18,\n",
        "        49,  22, 111, 115,  84,  80,  53])"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition.pca import *\n",
      "pca = PCA()\n",
      "pca.fit(data_train)\n",
      "print pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# REDUCE DIMENSIONALITY!\n",
      "n_dim = 50\n",
      "data_train_living = data_train_living[:, indices[:50]]\n",
      "data_train_dead = data_train_dead[:, indices[:50]]\n",
      "data_valid = data_valid[:, indices[:50]]\n",
      "print data_valid.shape, data_train_living.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(240386, 50) (312142, 50)\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_train = []\n",
      "C_valid = []\n",
      "y_valid = np.zeros((n_valid_combined, n_train_splits))\n",
      "for i in xrange(n_train_splits):\n",
      "    #ind_bal = np.arange(0, train_split+n_train_dead); np.random.shuffle(ind_bal);\n",
      "    data_bal = np.vstack((data_train_living[i*train_split:(i+1)*train_split, :], data_train_dead[:, :]) )\n",
      "    label_bal = np.vstack((np.zeros((train_split, 1)), np.ones((n_train_dead, 1))))\n",
      "    \n",
      "    print \"Hey!\\n\"\n",
      "    forests[i].fit(data_bal, label_bal[:, 0])\n",
      "    \n",
      "    print i\n",
      "    C_train.append(confusion_matrix(label_bal[:, 0], forests[i].predict(data_bal)))\n",
      "    get_ss(C_train[i])\n",
      "    print '\\n'\n",
      "    y = forests[i].predict(data_valid)\n",
      "    y_valid[:, i] = y\n",
      "    C_valid.append(confusion_matrix(labels_valid[:, 0], y))\n",
      "    get_ss(C_valid[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for i in xrange(len(C_valid)):\n",
      "    print i\n",
      "    get_ss(C_train[i])\n",
      "    print \n",
      "    get_ss(C_valid[i])\n",
      "    print\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "[[54622     6]\n",
        " [   49 54579]]\n",
        "\n",
        "Sensitiviy:  0.99910302409 \n",
        "Specificity:  0.999890166215\n",
        "\n",
        "[[75266 26112]\n",
        " [ 6737  9821]]\n",
        "\n",
        "Sensitiviy:  0.593127189274 \n",
        "Specificity:  0.742429323916\n",
        "\n",
        "\n",
        "1\n",
        "[[54614    14]\n",
        " [   43 54585]]\n",
        "\n",
        "Sensitiviy:  0.999212857875 \n",
        "Specificity:  0.999743721169\n",
        "\n",
        "[[75267 26111]\n",
        " [ 6887  9671]]\n",
        "\n",
        "Sensitiviy:  0.58406812417 \n",
        "Specificity:  0.74243918799\n",
        "\n",
        "\n",
        "2\n",
        "[[54619     9]\n",
        " [   32 54596]]\n",
        "\n",
        "Sensitiviy:  0.999414219814 \n",
        "Specificity:  0.999835249323\n",
        "\n",
        "[[71453 29925]\n",
        " [ 5955 10603]]\n",
        "\n",
        "Sensitiviy:  0.640355115352 \n",
        "Specificity:  0.704817613289\n",
        "\n",
        "\n",
        "3\n",
        "[[54623     5]\n",
        " [   57 54571]]\n",
        "\n",
        "Sensitiviy:  0.998956579044 \n",
        "Specificity:  0.999908471846\n",
        "\n",
        "[[75341 26037]\n",
        " [ 7105  9453]]\n",
        "\n",
        "Sensitiviy:  0.570902282884 \n",
        "Specificity:  0.743169129397\n",
        "\n",
        "\n",
        "4\n",
        "[[54622     6]\n",
        " [   51 54577]]\n",
        "\n",
        "Sensitiviy:  0.999066412829 \n",
        "Specificity:  0.999890166215\n",
        "\n",
        "[[75505 25873]\n",
        " [ 6768  9790]]\n",
        "\n",
        "Sensitiviy:  0.591254982486 \n",
        "Specificity:  0.744786837381\n",
        "\n",
        "\n",
        "5\n",
        "[[54619     9]\n",
        " [   47 54581]]\n",
        "\n",
        "Sensitiviy:  0.999139635352 \n",
        "Specificity:  0.999835249323\n",
        "\n",
        "[[74604 26774]\n",
        " [ 6884  9674]]\n",
        "\n",
        "Sensitiviy:  0.584249305472 \n",
        "Specificity:  0.735899307542\n",
        "\n",
        "\n",
        "6\n",
        "[[54622     6]\n",
        " [   39 54589]]\n",
        "\n",
        "Sensitiviy:  0.999286080398 \n",
        "Specificity:  0.999890166215\n",
        "\n",
        "[[74630 26748]\n",
        " [ 7064  9494]]\n",
        "\n",
        "Sensitiviy:  0.573378427346 \n",
        "Specificity:  0.736155773442\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(y_valid, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 205,
       "text": [
        "array([ 35933.,  35782.,  40528.,  35490.,  35663.,  36448.,  36242.])"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Majority vote?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comp_matrix = np.zeros((n_valid_combined, n_train_splits))\n",
      "for i in xrange(n_train_splits):\n",
      "    comp_matrix[:, i] = y_valid[:, i] == labels_valid[:, 0]\n",
      "print comp_matrix.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(117936, 7)\n"
       ]
      }
     ],
     "prompt_number": 273
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(comp_matrix, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 274,
       "text": [
        "array([ 81163.,  81262.,  77635.,  80204.,  81121.,  80790.,  80836.])"
       ]
      }
     ],
     "prompt_number": 274
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_1 = np.max(comp_matrix, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 307
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_1 = labels_valid.copy()\n",
      "y_1 = 1-y_1;\n",
      "y_1[s_1 == 1] = labels_valid[s_1 == 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 314
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_1.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 322,
       "text": [
        "(117936, 1)"
       ]
      }
     ],
     "prompt_number": 322
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_ss(metrics.confusion_matrix(labels_valid, y_1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[92956  8422]\n",
        " [ 1614 14944]]\n",
        "\n",
        "Sensitiviy:  0.902524459476 \n",
        "Specificity:  0.916924776579\n"
       ]
      }
     ],
     "prompt_number": 323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(np.sum(comp_matrix[labels_valid[:, 0] == 1 ,:], axis=1)>0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 278,
       "text": [
        "14944"
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combining the arrays makes predictions much much faster, but adds an overhead to find patients\n",
      "data_train_living = np.vstack(patient_train_living)\n",
      "data_train_dead = np.vstack(patient_train_dead)\n",
      "data_valid_living = np.vstack(patient_valid_living)\n",
      "data_valid_dead = np.vstack(patient_valid_dead)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate Ids of patients in the training and validation separated arrays\n",
      "id_train_living = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_train_living)\n",
      "                             ])\n",
      "id_train_dead = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_train_dead)\n",
      "                             ])\n",
      "id_valid_living = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_valid_living)\n",
      "                             ])\n",
      "id_valid_dead = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_valid_dead)\n",
      "                             ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 365
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict separtely for each of the sets\n",
      "pred_train_living = f.predict(data_train_living)\n",
      "pred_train_dead = f.predict(data_train_dead)\n",
      "pred_valid_dead = f.predict(data_valid_dead)\n",
      "pred_valid_living = f.predict(data_valid_living)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 423
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To easily access a specific patient\n",
      "def predp_train_living(i):\n",
      "    return pred_train_living[id_train_living[:, 0] == i]\n",
      "def predp_train_dead(i):\n",
      "    return pred_train_dead[id_train_dead[:, 0] == i]\n",
      "\n",
      "def predp_valid_living(i):\n",
      "    return pred_valid_living[id_valid_living[:, 0] == i]\n",
      "def predp_valid_dead(i):\n",
      "    return pred_valid_dead[id_valid_dead[:, 0] == i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 424
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_valid_living = np.array([np.array(predp_valid_living(i)) for i in xrange(len(patient_valid_living))])\n",
      "y_valid_dead = np.array([np.array(predp_valid_dead(i)) for i in xrange(len(patient_valid_dead))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 438
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_living = np.array([np.array(predp_train_living(i)) for i in xrange(len(patient_train_living))])\n",
      "y_train_dead = np.array([np.array(predp_train_dead(i)) for i in xrange(len(patient_train_dead))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 439
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_valid_living = np.array([np.mean(predp_valid_living(i)) for i in xrange(len(patient_valid_living))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 433
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(y_valid_living < 0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 435,
       "text": [
        "0.9609022556390977"
       ]
      }
     ],
     "prompt_number": 435
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for i in xrange(len(patient_valid_living)):\n",
      "    print np.mean(predp_valid_living(i))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "0.1875\n",
        "0.0229885057471\n",
        "0.433333333333\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0888888888889\n",
        "0.0\n",
        "0.138047138047\n",
        "0.144032921811\n",
        "0.14224137931\n",
        "0.0\n",
        "0.266666666667\n",
        "0.0\n",
        "0.0\n",
        "0.012323943662\n",
        "0.0487804878049\n",
        "0.312072892938\n",
        "0.0\n",
        "0.162303664921\n",
        "0.0\n",
        "0.0\n",
        "0.0760869565217\n",
        "0.0\n",
        "0.0\n",
        "0.148936170213\n",
        "0.0\n",
        "0.0197238658777\n",
        "0.0\n",
        "0.178489702517\n",
        "0.0\n",
        "0.0995260663507\n",
        "0.0120481927711\n",
        "0.0\n",
        "0.0\n",
        "0.00793650793651\n",
        "0.0\n",
        "0.0\n",
        "0.280961182994\n",
        "0.030534351145\n",
        "0.0\n",
        "0.0\n",
        "0.112676056338\n",
        "0.121428571429\n",
        "0.10071942446\n",
        "0.0\n",
        "0.0\n",
        "0.432098765432\n",
        "0.0\n",
        "0.0909090909091\n",
        "0.0\n",
        "0.0932203389831\n",
        "0.0\n",
        "0.070796460177\n",
        "0.0\n",
        "0.441176470588\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00520833333333\n",
        "0.0\n",
        "0.0149253731343\n",
        "0.0\n",
        "0.131661442006\n",
        "0.166666666667\n",
        "0.244956772334\n",
        "0.0280373831776\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.187145557656\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.19298245614\n",
        "0.282402528978\n",
        "0.264462809917\n",
        "0.0247933884298\n",
        "0.048275862069\n",
        "0.0\n",
        "0.0\n",
        "0.328125\n",
        "0.0965732087227\n",
        "0.149377593361\n",
        "0.0267857142857\n",
        "0.0449438202247\n",
        "0.0\n",
        "0.0\n",
        "0.0194174757282\n",
        "0.192307692308\n",
        "0.0142857142857\n",
        "0.0\n",
        "0.0\n",
        "0.0579710144928\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.217948717949\n",
        "0.471074380165\n",
        "0.0\n",
        "0.0724637681159\n",
        "0.0\n",
        "0.227848101266\n",
        "0.012987012987\n",
        "0.244897959184\n",
        "0.0\n",
        "0.0\n",
        "0.0377358490566\n",
        "0.0\n",
        "0.0\n",
        "0.226666666667\n",
        "0.0\n",
        "0.170212765957\n",
        "0.0\n",
        "0.0\n",
        "0.03\n",
        "0.0\n",
        "0.0\n",
        "0.188449848024\n",
        "0.0\n",
        "0.271428571429\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.155172413793\n",
        "0.148648648649\n",
        "0.25\n",
        "0.0\n",
        "0.0\n",
        "0.125\n",
        "0.0526315789474\n",
        "0.0\n",
        "0.00701754385965\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.016393442623\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.190476190476\n",
        "0.194092827004\n",
        "0.0856031128405\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.388838060384\n",
        "0.0\n",
        "0.0195652173913\n",
        "0.0155642023346\n",
        "0.285714285714\n",
        "0.333333333333\n",
        "0.246575342466\n",
        "0.107692307692\n",
        "0.218181818182\n",
        "0.0\n",
        "0.0\n",
        "0.0326086956522\n",
        "0.0\n",
        "0.072864321608\n",
        "0.0\n",
        "0.0191570881226\n",
        "0.129909365559\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0736842105263\n",
        "0.0751252086811\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00461538461538\n",
        "0.00149700598802\n",
        "0.00247524752475\n",
        "0.0\n",
        "0.196335078534\n",
        "0.0567164179104\n",
        "0.0\n",
        "0.0\n",
        "0.166666666667\n",
        "0.178947368421\n",
        "0.390438247012\n",
        "0.00847457627119\n",
        "0.0\n",
        "0.207207207207\n",
        "0.0980392156863\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.16393442623\n",
        "0.0352760736196\n",
        "0.0\n",
        "0.0333333333333\n",
        "0.0\n",
        "0.036036036036\n",
        "0.0637362637363\n",
        "0.0\n",
        "0.0970873786408\n",
        "0.0\n",
        "0.0361445783133\n",
        "0.05\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.121951219512\n",
        "0.0\n",
        "0.0\n",
        "0.197674418605\n",
        "0.0155440414508\n",
        "0.0\n",
        "0.0120192307692\n",
        "0.0309734513274\n",
        "0.0823529411765\n",
        "0.0\n",
        "0.0181818181818\n",
        "0.0703125\n",
        "0.0\n",
        "0.162629757785\n",
        "0.0\n",
        "0.0\n",
        "0.0121951219512\n",
        "0.00970873786408\n",
        "0.0583333333333\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.116438356164\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0645161290323\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.122302158273\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0505836575875\n",
        "0.0173913043478\n",
        "0.109022556391\n",
        "0.133333333333\n",
        "0.0\n",
        "0.115384615385\n",
        "0.0\n",
        "0.0334728033473\n",
        "0.277777777778\n",
        "0.00819672131148\n",
        "0.0484848484848\n",
        "0.00595238095238\n",
        "0.12\n",
        "0.0\n",
        "0.222921914358\n",
        "0.0\n",
        "0.0\n",
        "0.0188679245283\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0208333333333\n",
        "0.0890410958904\n",
        "0.0196078431373\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.105734767025\n",
        "0.265402843602\n",
        "0.0\n",
        "0.416666666667\n",
        "0.0740740740741\n",
        "0.0\n",
        "0.0902255639098\n",
        "0.0\n",
        "0.0\n",
        "0.103448275862\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.359154929577\n",
        "0.0\n",
        "0.0\n",
        "0.06\n",
        "0.0552995391705\n",
        "0.0\n",
        "0.379562043796\n",
        "0.165876777251\n",
        "0.256013745704\n",
        "0.0521172638436\n",
        "0.0\n",
        "0.182186234818\n",
        "0.262295081967\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0294117647059\n",
        "0.0\n",
        "0.364341085271\n",
        "0.133333333333\n",
        "0.0\n",
        "0.149028077754\n",
        "0.0\n",
        "0.172413793103\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0125\n",
        "0.211267605634\n",
        "0.0496688741722\n",
        "0.0149253731343\n",
        "0.0173010380623\n",
        "0.0\n",
        "0.300429184549\n",
        "0.0\n",
        "0.0\n",
        "0.212543554007\n",
        "0.0375\n",
        "0.0\n",
        "0.0839694656489\n",
        "0.0\n",
        "0.00136612021858\n",
        "0.00970873786408\n",
        "0.352813852814\n",
        "0.073732718894\n",
        "0.04\n",
        "0.0188679245283\n",
        "0.0\n",
        "0.117647058824\n",
        "0.0\n",
        "0.2\n",
        "0.0736196319018\n",
        "0.0\n",
        "0.0210970464135\n",
        "0.0434782608696\n",
        "0.0\n",
        "0.223880597015\n",
        "0.00363636363636\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.225\n",
        "0.0\n",
        "0.0\n",
        "0.10752688172\n",
        "0.0\n",
        "0.0\n",
        "0.0377358490566\n",
        "0.0555555555556\n",
        "0.015625\n",
        "0.0384615384615\n",
        "0.16835016835\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.181818181818\n",
        "0.0\n",
        "0.0\n",
        "0.089715536105\n",
        "0.025\n",
        "0.0\n",
        "0.00540540540541\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.11320754717\n",
        "0.0\n",
        "0.0\n",
        "0.0617283950617\n",
        "0.397402597403\n",
        "0.005\n",
        "0.0229885057471\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0696864111498\n",
        "0.0\n",
        "0.00411522633745\n",
        "0.265306122449\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.303825956489\n",
        "0.0384615384615\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0319634703196\n",
        "0.0344827586207\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.5\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00434782608696\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0277777777778\n",
        "0.192592592593\n",
        "0.0\n",
        "0.0778947368421\n",
        "0.0204081632653\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00772200772201\n",
        "0.0\n",
        "0.127272727273\n",
        "0.392572944297\n",
        "0.043795620438\n",
        "0.0\n",
        "0.0\n",
        "0.00819672131148\n",
        "0.0406504065041\n",
        "0.0125\n",
        "0.0\n",
        "0.142857142857\n",
        "0.0\n",
        "0.093023255814\n",
        "0.0\n",
        "0.0\n",
        "0.110655737705\n",
        "0.00220750551876\n",
        "0.0\n",
        "0.0137457044674\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.145695364238\n",
        "0.0111111111111\n",
        "0.0\n",
        "0.0606060606061\n",
        "0.0172413793103\n",
        "0.00540540540541\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.136801541426\n",
        "0.0\n",
        "0.0\n",
        "0.16814159292\n",
        "0.0520487264673\n",
        "0.0\n",
        "0.0551181102362\n",
        "0.135135135135\n",
        "0.183098591549\n",
        "0.205128205128\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0845070422535\n",
        "0.018691588785\n",
        "0.0227272727273\n",
        "0.0901639344262\n",
        "0.0\n",
        "0.0176470588235\n",
        "0.0108695652174\n",
        "0.0\n",
        "0.185897435897\n",
        "0.0\n",
        "0.22972972973\n",
        "0.217391304348\n",
        "0.0570469798658\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.197580645161\n",
        "0.224880382775\n",
        "0.0666666666667\n",
        "0.0178571428571\n",
        "0.0952380952381\n",
        "0.0491803278689\n",
        "0.138888888889\n",
        "0.386666666667\n",
        "0.0\n",
        "0.120728929385\n",
        "0.0223880597015\n",
        "0.0\n",
        "0.0\n",
        "0.0432432432432\n",
        "0.0170454545455\n",
        "0.122270742358\n",
        "0.0\n",
        "0.336956521739\n",
        "0.0062893081761\n",
        "0.0\n",
        "0.0\n",
        "0.131578947368\n",
        "0.393939393939\n",
        "0.0\n",
        "0.0224215246637\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.046875\n",
        "0.0\n",
        "0.158536585366\n",
        "0.0\n",
        "0.0\n",
        "0.025069637883\n",
        "0.191044776119\n",
        "0.025641025641\n",
        "0.158878504673\n",
        "0.0\n",
        "0.025\n",
        "0.025974025974\n",
        "0.0384615384615\n",
        "0.013698630137\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.182978723404\n",
        "0.0\n",
        "0.0919540229885\n",
        "0.0609318996416\n",
        "0.122935779817\n",
        "0.0\n",
        "0.0666666666667\n",
        "0.00369003690037\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.204819277108\n",
        "0.0327868852459\n",
        "0.15\n",
        "0.0240384615385\n",
        "0.047619047619\n",
        "0.0\n",
        "0.016393442623\n",
        "0.00540540540541\n",
        "0.0\n",
        "0.0\n",
        "0.0818181818182\n",
        "0.0\n",
        "0.245901639344\n",
        "0.046511627907\n",
        "0.269911504425\n",
        "0.0\n",
        "0.0217391304348\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.118644067797\n",
        "0.0\n",
        "0.0\n",
        "0.731707317073\n",
        "0.310160427807\n",
        "0.120689655172\n",
        "0.0\n",
        "0.0268096514745\n",
        "0.00881057268722\n",
        "0.0\n",
        "0.0\n",
        "0.270358306189\n",
        "0.0\n",
        "0.152173913043\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.089527027027\n",
        "0.0\n",
        "0.156862745098\n",
        "0.0\n",
        "0.0277777777778\n",
        "0.0324675324675\n",
        "0.252032520325\n",
        "0.0140845070423\n",
        "0.0736842105263\n",
        "0.648648648649\n",
        "0.0\n",
        "0.0\n",
        "0.283636363636\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0177304964539\n",
        "0.288025889968\n",
        "0.00719424460432\n",
        "0.0\n",
        "0.0325581395349\n",
        "0.0243902439024\n",
        "0.0078431372549\n",
        "0.0\n",
        "0.0710227272727\n",
        "0.0\n",
        "0.0\n",
        "0.193103448276\n",
        "0.230182926829\n",
        "0.0\n",
        "0.0\n",
        "0.0425531914894\n",
        "0.0\n",
        "0.281176470588\n",
        "0.0128755364807\n",
        "0.388190954774\n",
        "0.0994623655914\n",
        "0.0368098159509\n",
        "0.0\n",
        "0.0\n",
        "0.0234899328859\n",
        "0.0\n",
        "0.0\n",
        "0.0384615384615\n",
        "0.0\n",
        "0.0555555555556\n",
        "0.0\n",
        "0.0\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 413
    }
   ],
   "metadata": {}
  }
 ]
}