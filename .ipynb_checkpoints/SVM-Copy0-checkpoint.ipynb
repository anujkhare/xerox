{
 "metadata": {
  "name": "",
  "signature": "sha256:2318d115ffa82c7231d48501080afa2f59ff01cdb07a2ab89e40888ceca98f96"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%vimception"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Line magic function `%vimception` not found.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import pandas\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_age = pd.read_csv('data_train/id_age_train.csv')\n",
      "df_labels = pd.read_csv('data_train/id_label_train.csv')\n",
      "df_vitals = pd.read_csv('data_train/id_time_vitals_train.csv')\n",
      "df_labs = pd.read_csv('data_train/id_time_labs_train.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_vitals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>ID</th>\n",
        "      <th>TIME</th>\n",
        "      <th>V1</th>\n",
        "      <th>V2</th>\n",
        "      <th>V3</th>\n",
        "      <th>V4</th>\n",
        "      <th>V5</th>\n",
        "      <th>V6</th>\n",
        "      <th>ICU</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0     </th>\n",
        "      <td>    1</td>\n",
        "      <td>      0</td>\n",
        "      <td>  86</td>\n",
        "      <td>  49</td>\n",
        "      <td>  70</td>\n",
        "      <td>NaN</td>\n",
        "      <td>  87</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   4320</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  70</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   5646</td>\n",
        "      <td>  91</td>\n",
        "      <td>  58</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 96.6</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   5703</td>\n",
        "      <td> 140</td>\n",
        "      <td>  73</td>\n",
        "      <td>  91</td>\n",
        "      <td> 32</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   6342</td>\n",
        "      <td> 139</td>\n",
        "      <td>  90</td>\n",
        "      <td> 107</td>\n",
        "      <td> 29</td>\n",
        "      <td> 101</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   6609</td>\n",
        "      <td> 152</td>\n",
        "      <td>  75</td>\n",
        "      <td> 109</td>\n",
        "      <td> 30</td>\n",
        "      <td> 101</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   6894</td>\n",
        "      <td> 140</td>\n",
        "      <td>  79</td>\n",
        "      <td>  84</td>\n",
        "      <td>NaN</td>\n",
        "      <td>  98</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   6957</td>\n",
        "      <td> 140</td>\n",
        "      <td>  72</td>\n",
        "      <td> 108</td>\n",
        "      <td> 31</td>\n",
        "      <td> 101</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   7511</td>\n",
        "      <td> 132</td>\n",
        "      <td>  68</td>\n",
        "      <td> 110</td>\n",
        "      <td> 31</td>\n",
        "      <td>  95</td>\n",
        "      <td> 95.2</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9     </th>\n",
        "      <td>    1</td>\n",
        "      <td>   8372</td>\n",
        "      <td> 139</td>\n",
        "      <td>  70</td>\n",
        "      <td> 106</td>\n",
        "      <td> 31</td>\n",
        "      <td>  97</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10    </th>\n",
        "      <td>    1</td>\n",
        "      <td>   9297</td>\n",
        "      <td> 134</td>\n",
        "      <td>  68</td>\n",
        "      <td> 105</td>\n",
        "      <td> 31</td>\n",
        "      <td>  99</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  10213</td>\n",
        "      <td> 134</td>\n",
        "      <td>  68</td>\n",
        "      <td> 105</td>\n",
        "      <td> 31</td>\n",
        "      <td>  99</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  11079</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  11123</td>\n",
        "      <td> 132</td>\n",
        "      <td>  67</td>\n",
        "      <td>  96</td>\n",
        "      <td> 31</td>\n",
        "      <td> 100</td>\n",
        "      <td> 93.1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  12015</td>\n",
        "      <td> 132</td>\n",
        "      <td>  69</td>\n",
        "      <td>  92</td>\n",
        "      <td> 31</td>\n",
        "      <td> 101</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  12889</td>\n",
        "      <td> 127</td>\n",
        "      <td>  72</td>\n",
        "      <td>  92</td>\n",
        "      <td> 31</td>\n",
        "      <td>  97</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  13798</td>\n",
        "      <td> 124</td>\n",
        "      <td>  65</td>\n",
        "      <td>  86</td>\n",
        "      <td> 31</td>\n",
        "      <td>  97</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  14713</td>\n",
        "      <td> 119</td>\n",
        "      <td>  67</td>\n",
        "      <td>  86</td>\n",
        "      <td> 31</td>\n",
        "      <td>  97</td>\n",
        "      <td> 90.6</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  15577</td>\n",
        "      <td> 106</td>\n",
        "      <td>  64</td>\n",
        "      <td>  80</td>\n",
        "      <td> 30</td>\n",
        "      <td>  97</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  16523</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  72</td>\n",
        "      <td> 31</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  17415</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  94</td>\n",
        "      <td> 31</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  17581</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  77</td>\n",
        "      <td>NaN</td>\n",
        "      <td> 100</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  17644</td>\n",
        "      <td> 141</td>\n",
        "      <td> 116</td>\n",
        "      <td>  77</td>\n",
        "      <td> 24</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  17893</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  17938</td>\n",
        "      <td> 149</td>\n",
        "      <td>  74</td>\n",
        "      <td>  73</td>\n",
        "      <td> 31</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  18306</td>\n",
        "      <td> 137</td>\n",
        "      <td>  85</td>\n",
        "      <td>  73</td>\n",
        "      <td> 31</td>\n",
        "      <td>  87</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  21903</td>\n",
        "      <td> 124</td>\n",
        "      <td>  62</td>\n",
        "      <td>  84</td>\n",
        "      <td> 31</td>\n",
        "      <td>  82</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  22787</td>\n",
        "      <td> 112</td>\n",
        "      <td>  52</td>\n",
        "      <td>  85</td>\n",
        "      <td> 31</td>\n",
        "      <td>  78</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  23670</td>\n",
        "      <td> 117</td>\n",
        "      <td>  56</td>\n",
        "      <td>  88</td>\n",
        "      <td> 31</td>\n",
        "      <td>  71</td>\n",
        "      <td> 90.7</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29    </th>\n",
        "      <td>    1</td>\n",
        "      <td>  24584</td>\n",
        "      <td> 116</td>\n",
        "      <td>  51</td>\n",
        "      <td>  93</td>\n",
        "      <td> 31</td>\n",
        "      <td>  70</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628407</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 107386</td>\n",
        "      <td> 114</td>\n",
        "      <td>  71</td>\n",
        "      <td>  67</td>\n",
        "      <td> 15</td>\n",
        "      <td>  96</td>\n",
        "      <td> 97.7</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628408</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 111004</td>\n",
        "      <td> 124</td>\n",
        "      <td>  70</td>\n",
        "      <td>  72</td>\n",
        "      <td> 15</td>\n",
        "      <td>  95</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628409</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 114605</td>\n",
        "      <td> 117</td>\n",
        "      <td>  69</td>\n",
        "      <td>  67</td>\n",
        "      <td> 18</td>\n",
        "      <td>  96</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628410</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 115010</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628411</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 118210</td>\n",
        "      <td> 114</td>\n",
        "      <td>  72</td>\n",
        "      <td>  75</td>\n",
        "      <td> 18</td>\n",
        "      <td>  95</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628412</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 121812</td>\n",
        "      <td> 114</td>\n",
        "      <td>  84</td>\n",
        "      <td>  79</td>\n",
        "      <td> 24</td>\n",
        "      <td>  95</td>\n",
        "      <td> 97.1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628413</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 125428</td>\n",
        "      <td> 126</td>\n",
        "      <td>  72</td>\n",
        "      <td>  76</td>\n",
        "      <td> 22</td>\n",
        "      <td>  97</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628414</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 129024</td>\n",
        "      <td> 124</td>\n",
        "      <td>  69</td>\n",
        "      <td>  72</td>\n",
        "      <td> 22</td>\n",
        "      <td>  98</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628415</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 132575</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>  97</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628416</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 134562</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  66</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628417</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 134989</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  63</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628418</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 136184</td>\n",
        "      <td> 121</td>\n",
        "      <td>  71</td>\n",
        "      <td>  74</td>\n",
        "      <td> 21</td>\n",
        "      <td>  98</td>\n",
        "      <td> 97.4</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628419</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 143397</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  68</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628420</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 150035</td>\n",
        "      <td> 150</td>\n",
        "      <td>  88</td>\n",
        "      <td>  75</td>\n",
        "      <td> 13</td>\n",
        "      <td>  96</td>\n",
        "      <td> 98.1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628421</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 150727</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  63</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628422</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 155715</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  72</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628423</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 164985</td>\n",
        "      <td> 124</td>\n",
        "      <td>  78</td>\n",
        "      <td>  69</td>\n",
        "      <td> 25</td>\n",
        "      <td>  94</td>\n",
        "      <td> 98.0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628424</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 179398</td>\n",
        "      <td> 125</td>\n",
        "      <td>  77</td>\n",
        "      <td>  71</td>\n",
        "      <td> 21</td>\n",
        "      <td>  94</td>\n",
        "      <td> 98.7</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628425</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 193776</td>\n",
        "      <td> 119</td>\n",
        "      <td>  85</td>\n",
        "      <td>  62</td>\n",
        "      <td> 22</td>\n",
        "      <td>  93</td>\n",
        "      <td> 98.3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628426</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 199795</td>\n",
        "      <td> 119</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 98.3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628427</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 208218</td>\n",
        "      <td> 124</td>\n",
        "      <td>  80</td>\n",
        "      <td>  76</td>\n",
        "      <td> 19</td>\n",
        "      <td>  92</td>\n",
        "      <td> 98.1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628428</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 222611</td>\n",
        "      <td> 127</td>\n",
        "      <td>  92</td>\n",
        "      <td>  73</td>\n",
        "      <td> 17</td>\n",
        "      <td>  95</td>\n",
        "      <td> 98.7</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628429</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 236974</td>\n",
        "      <td> 142</td>\n",
        "      <td>  87</td>\n",
        "      <td>  55</td>\n",
        "      <td> 17</td>\n",
        "      <td>  96</td>\n",
        "      <td> 98.2</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628430</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 251404</td>\n",
        "      <td> 130</td>\n",
        "      <td>  78</td>\n",
        "      <td>  56</td>\n",
        "      <td> 17</td>\n",
        "      <td>  95</td>\n",
        "      <td> 98.0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628431</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 265781</td>\n",
        "      <td> 132</td>\n",
        "      <td>  79</td>\n",
        "      <td>  59</td>\n",
        "      <td> 17</td>\n",
        "      <td>  94</td>\n",
        "      <td> 98.0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628432</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 280223</td>\n",
        "      <td> 112</td>\n",
        "      <td>  50</td>\n",
        "      <td>  58</td>\n",
        "      <td> 17</td>\n",
        "      <td>  95</td>\n",
        "      <td> 97.1</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628433</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 290430</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628434</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 296075</td>\n",
        "      <td> 105</td>\n",
        "      <td>  59</td>\n",
        "      <td>  65</td>\n",
        "      <td> 17</td>\n",
        "      <td>  98</td>\n",
        "      <td> 98.3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628435</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 299096</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>  93</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>628436</th>\n",
        "      <td> 3594</td>\n",
        "      <td> 309027</td>\n",
        "      <td> 109</td>\n",
        "      <td>  68</td>\n",
        "      <td>  90</td>\n",
        "      <td>NaN</td>\n",
        "      <td>  94</td>\n",
        "      <td> 98.0</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>628437 rows \u00d7 9 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "          ID    TIME   V1   V2   V3  V4   V5    V6  ICU\n",
        "0          1       0   86   49   70 NaN   87   NaN    0\n",
        "1          1    4320  NaN  NaN   70 NaN  NaN   NaN    0\n",
        "2          1    5646   91   58  NaN NaN  NaN  96.6    1\n",
        "3          1    5703  140   73   91  32  NaN   NaN    1\n",
        "4          1    6342  139   90  107  29  101   NaN    1\n",
        "5          1    6609  152   75  109  30  101   NaN    1\n",
        "6          1    6894  140   79   84 NaN   98   NaN    1\n",
        "7          1    6957  140   72  108  31  101   NaN    1\n",
        "8          1    7511  132   68  110  31   95  95.2    1\n",
        "9          1    8372  139   70  106  31   97   NaN    1\n",
        "10         1    9297  134   68  105  31   99   NaN    1\n",
        "11         1   10213  134   68  105  31   99   NaN    1\n",
        "12         1   11079  NaN  NaN  NaN NaN  NaN   NaN    1\n",
        "13         1   11123  132   67   96  31  100  93.1    1\n",
        "14         1   12015  132   69   92  31  101   NaN    1\n",
        "15         1   12889  127   72   92  31   97   NaN    1\n",
        "16         1   13798  124   65   86  31   97   NaN    1\n",
        "17         1   14713  119   67   86  31   97  90.6    1\n",
        "18         1   15577  106   64   80  30   97   NaN    1\n",
        "19         1   16523  NaN  NaN   72  31  NaN   NaN    1\n",
        "20         1   17415  NaN  NaN   94  31  NaN   NaN    1\n",
        "21         1   17581  NaN  NaN   77 NaN  100   NaN    1\n",
        "22         1   17644  141  116   77  24  NaN   NaN    1\n",
        "23         1   17893  NaN  NaN  NaN NaN  NaN   NaN    1\n",
        "24         1   17938  149   74   73  31  NaN   NaN    1\n",
        "25         1   18306  137   85   73  31   87   NaN    1\n",
        "26         1   21903  124   62   84  31   82   NaN    1\n",
        "27         1   22787  112   52   85  31   78   NaN    1\n",
        "28         1   23670  117   56   88  31   71  90.7    1\n",
        "29         1   24584  116   51   93  31   70   NaN    1\n",
        "...      ...     ...  ...  ...  ...  ..  ...   ...  ...\n",
        "628407  3594  107386  114   71   67  15   96  97.7    1\n",
        "628408  3594  111004  124   70   72  15   95   NaN    1\n",
        "628409  3594  114605  117   69   67  18   96   NaN    1\n",
        "628410  3594  115010  NaN  NaN  NaN NaN  NaN   NaN    1\n",
        "628411  3594  118210  114   72   75  18   95   NaN    1\n",
        "628412  3594  121812  114   84   79  24   95  97.1    1\n",
        "628413  3594  125428  126   72   76  22   97   NaN    1\n",
        "628414  3594  129024  124   69   72  22   98   NaN    1\n",
        "628415  3594  132575  NaN  NaN  NaN NaN   97   NaN    1\n",
        "628416  3594  134562  NaN  NaN   66 NaN  NaN   NaN    1\n",
        "628417  3594  134989  NaN  NaN   63 NaN  NaN   NaN    1\n",
        "628418  3594  136184  121   71   74  21   98  97.4    1\n",
        "628419  3594  143397  NaN  NaN   68 NaN  NaN   NaN    1\n",
        "628420  3594  150035  150   88   75  13   96  98.1    1\n",
        "628421  3594  150727  NaN  NaN   63 NaN  NaN   NaN    1\n",
        "628422  3594  155715  NaN  NaN   72 NaN  NaN   NaN    1\n",
        "628423  3594  164985  124   78   69  25   94  98.0    1\n",
        "628424  3594  179398  125   77   71  21   94  98.7    1\n",
        "628425  3594  193776  119   85   62  22   93  98.3    1\n",
        "628426  3594  199795  119  NaN  NaN NaN  NaN  98.3    1\n",
        "628427  3594  208218  124   80   76  19   92  98.1    1\n",
        "628428  3594  222611  127   92   73  17   95  98.7    1\n",
        "628429  3594  236974  142   87   55  17   96  98.2    1\n",
        "628430  3594  251404  130   78   56  17   95  98.0    1\n",
        "628431  3594  265781  132   79   59  17   94  98.0    1\n",
        "628432  3594  280223  112   50   58  17   95  97.1    1\n",
        "628433  3594  290430  NaN  NaN  NaN NaN  NaN   NaN    1\n",
        "628434  3594  296075  105   59   65  17   98  98.3    1\n",
        "628435  3594  299096  NaN  NaN  NaN NaN   93   NaN    1\n",
        "628436  3594  309027  109   68   90 NaN   94  98.0    1\n",
        "\n",
        "[628437 rows x 9 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vitals = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(6):\n",
      "    col = 'V' + str(i)\n",
      "    v = df_vitals.ix[]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Impute with mean\n",
      "df_vitals_mean = df_vitals.copy()\n",
      "for i in xrange(1, 7):\n",
      "    col = 'V' + str(i)\n",
      "    index = df_vitals_mean.ix[:, col].isnull()\n",
      "    df_vitals_mean.ix[index, col] = df_vitals_mean.ix[index^1, col].mean()\n",
      "df_labs_mean = df_labs.copy()\n",
      "for i in xrange(1, 26):\n",
      "    col = 'L' + str(i)\n",
      "    index = df_labs_mean.ix[:, col].isnull()\n",
      "    df_labs_mean.ix[index, col] = df_labs_mean.ix[index^1, col].mean()\n",
      "# Combined values imputed with mean\n",
      "patient_combined_mean = [np.append(np.array(df_vitals_mean.ix[df_vitals_mean['ID'] == ID, 2:8]),\n",
      "                                   np.array(df_labs_mean.ix[df_labs_mean['ID'] == ID, 2:27]), axis=1)\n",
      "                         for ID in df_labels['ID'].unique()                        \n",
      "                         ]\n",
      "\n",
      "labels_combined = list(df_labels.ix[:, 'LABEL'])\n",
      "# Windows!\n",
      "window_size = 10\n",
      "stride = 1\n",
      "n_features = 31\n",
      "n_stats = 4\n",
      "data = patient_combined_mean\n",
      "labels = []\n",
      "window_stats = []\n",
      "\n",
      "for i in xrange(len(data)):\n",
      "    ts = data[i]\n",
      "    n_windows = ts.shape[0] - window_size + 1 # NO STRIDE!!\n",
      "    \n",
      "    if(n_windows <= 0):\n",
      "        #window_stats.append(None)    # We want to keep 1-1 mapping with patients..\n",
      "        continue\n",
      "    \n",
      "    l = np.zeros((n_windows, n_stats * n_features))\n",
      "    \n",
      "    for j in xrange(0, n_windows, stride):   # NO STRIDE!\n",
      "        window = ts[j : window_size+j, :] \n",
      "        l[j, :n_features] = np.nanmean(window, axis=0)\n",
      "        l[j, n_features:2*n_features] = np.nanvar(window, axis=0)\n",
      "        l[j, 2*n_features:3*n_features] = np.nanmin(window, axis=0)\n",
      "        l[j, 3*n_features:4*n_features] = np.nanmax(window, axis=0)\n",
      "    \n",
      "    window_stats.append(l)\n",
      "    labels.append(labels_combined[i])\n",
      "\n",
      "# SET teh global variable\n",
      "window_stats_patient = window_stats\n",
      "labels = np.array(labels)\n",
      "print len(window_stats_patient)\n",
      "print len(labels)\n",
      "### Save the data\n",
      "np.save('/root/code/xerox/windows/windows_combined', window_stats_patient)\n",
      "np.save('/root/code/xerox/windows/labels_combined', labels)\n",
      "### No. of patients with NO WINDOW\n",
      "print sum([window_stats_patient_dead[i] == None for i in xrange(n_dead)])\n",
      "print sum([window_stats_patient_living[i] == None for i in xrange(n_living)])\n",
      "print len(window_stats_patient_dead)\n",
      "print len(window_stats_patient_living)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_combined = np.load('/root/code/xerox/windows/windows_combined.npy')\n",
      "labels_combined = np.load('/root/code/xerox/windows/labels_combined.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(labels_combined)\n",
      "print len(data_combined)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3546\n",
        "3546\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# RANDOM FOREST"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = np.arange(0, len(labels_combined))\n",
      "np.random.shuffle(ind)\n",
      "N = len(ind)\n",
      "split = 0.8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N_train = int(split * N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind_train = ind[:N_train]\n",
      "ind_valid = ind[N_train:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Divide into sets and then stack to get dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generte the data matrix for stacked data\n",
      "data_train = np.vstack(data_combined[ind_train])\n",
      "data_valid = np.vstack(data_combined[ind_valid])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generate the labels for the stacked data\n",
      "labels_train = np.vstack([np.zeros((data_combined[i].shape[0], 1)) + labels_combined[i] for i in ind_train])\n",
      "labels_valid = np.vstack([np.zeros((data_combined[i].shape[0], 1)) + labels_combined[i] for i in ind_valid])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_train_dead = int(sum(labels_train)[0])\n",
      "n_train_living = int(sum(1 - labels_train)[0])\n",
      "n_train_combined = n_train_living + n_train_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_valid_dead = int(sum(labels_valid)[0])\n",
      "n_valid_living = int(sum(1 - labels_valid)[0])\n",
      "n_valid_combined = n_valid_living + n_valid_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print n_train_dead, n_train_living, labels_train.shape[0]\n",
      "print n_valid_dead, n_valid_living, labels_valid.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "56828 415719 472547\n",
        "14358 109327 123685\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate list of living and dead patients in train and validation dtta"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patient_train_living = [data_combined[i] for i in ind_train if labels_combined[i] ==0]\n",
      "patient_train_dead = [data_combined[i] for i in ind_train if labels_combined[i] ==1]\n",
      "patient_valid_living = [data_combined[i] for i in ind_valid if labels_combined[i] ==0]\n",
      "patient_valid_dead = [data_combined[i] for i in ind_valid if labels_combined[i] ==1]\n",
      "print len(patient_valid_living), len(patient_valid_dead)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "665 45\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_train_living = np.vstack(patient_train_living)\n",
      "data_train_dead = np.vstack(patient_train_dead)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Confusion!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def get_ss(confusion):\n",
      "    tn = confusion[0, 0]  # label=0, pred=0\n",
      "    fp = confusion[0, 1]  # label=0, pred=1\n",
      "    \n",
      "    fn = confusion[1, 0]  # label=1, pred=0\n",
      "    tp = confusion[1, 1]  # label=1, pred=1\n",
      "    \n",
      "    sens = (1.0*tp/(tp+fn))\n",
      "    spec = (1.0*tn/(tn+fp))\n",
      "    \n",
      "    print confusion\n",
      "    print\n",
      "    #print tp, fp, fn, tn, sens, spec\n",
      "    print 'Sensitiviy: ', sens, '\\nSpecificity: ', spec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Split training set into equal dead and living parts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_train_living/ n_train_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "7"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_split = int (n_train_living / (n_train_living / n_train_dead))\n",
      "n_train_splits = int (n_train_living / n_train_dead)\n",
      "print n_train_splits, train_split, n_train_splits * train_split, n_train_combined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 59388 415716 472547\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forests = [svm.NuSVC(kernel='poly', degree=2, gamma=10**(-1.7), probability=True)] * n_train_splits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forests[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "NuSVC(cache_size=200, coef0=0.0, degree=2, gamma=0.0199526231497,\n",
        "   kernel='poly', max_iter=-1, nu=0.5, probability=True, random_state=None,\n",
        "   shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for i in xrange(len(C_valid)):\n",
      "    print i\n",
      "    get_ss(C_train[i])\n",
      "    print \n",
      "    get_ss(C_valid[i])\n",
      "    print\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "[[54622     6]\n",
        " [   49 54579]]\n",
        "\n",
        "Sensitiviy:  0.99910302409 \n",
        "Specificity:  0.999890166215\n",
        "\n",
        "[[75266 26112]\n",
        " [ 6737  9821]]\n",
        "\n",
        "Sensitiviy:  0.593127189274 \n",
        "Specificity:  0.742429323916\n",
        "\n",
        "\n",
        "1\n",
        "[[54614    14]\n",
        " [   43 54585]]\n",
        "\n",
        "Sensitiviy:  0.999212857875 \n",
        "Specificity:  0.999743721169\n",
        "\n",
        "[[75267 26111]\n",
        " [ 6887  9671]]\n",
        "\n",
        "Sensitiviy:  0.58406812417 \n",
        "Specificity:  0.74243918799\n",
        "\n",
        "\n",
        "2\n",
        "[[54619     9]\n",
        " [   32 54596]]\n",
        "\n",
        "Sensitiviy:  0.999414219814 \n",
        "Specificity:  0.999835249323\n",
        "\n",
        "[[71453 29925]\n",
        " [ 5955 10603]]\n",
        "\n",
        "Sensitiviy:  0.640355115352 \n",
        "Specificity:  0.704817613289\n",
        "\n",
        "\n",
        "3\n",
        "[[54623     5]\n",
        " [   57 54571]]\n",
        "\n",
        "Sensitiviy:  0.998956579044 \n",
        "Specificity:  0.999908471846\n",
        "\n",
        "[[75341 26037]\n",
        " [ 7105  9453]]\n",
        "\n",
        "Sensitiviy:  0.570902282884 \n",
        "Specificity:  0.743169129397\n",
        "\n",
        "\n",
        "4\n",
        "[[54622     6]\n",
        " [   51 54577]]\n",
        "\n",
        "Sensitiviy:  0.999066412829 \n",
        "Specificity:  0.999890166215\n",
        "\n",
        "[[75505 25873]\n",
        " [ 6768  9790]]\n",
        "\n",
        "Sensitiviy:  0.591254982486 \n",
        "Specificity:  0.744786837381\n",
        "\n",
        "\n",
        "5\n",
        "[[54619     9]\n",
        " [   47 54581]]\n",
        "\n",
        "Sensitiviy:  0.999139635352 \n",
        "Specificity:  0.999835249323\n",
        "\n",
        "[[74604 26774]\n",
        " [ 6884  9674]]\n",
        "\n",
        "Sensitiviy:  0.584249305472 \n",
        "Specificity:  0.735899307542\n",
        "\n",
        "\n",
        "6\n",
        "[[54622     6]\n",
        " [   39 54589]]\n",
        "\n",
        "Sensitiviy:  0.999286080398 \n",
        "Specificity:  0.999890166215\n",
        "\n",
        "[[74630 26748]\n",
        " [ 7064  9494]]\n",
        "\n",
        "Sensitiviy:  0.573378427346 \n",
        "Specificity:  0.736155773442\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_train = []\n",
      "C_valid = []\n",
      "y_valid = np.zeros((n_valid_combined, n_train_splits))\n",
      "for i in xrange(n_train_splits):\n",
      "    #ind_bal = np.arange(0, train_split+n_train_dead); np.random.shuffle(ind_bal);\n",
      "    data_bal = np.vstack((data_train_living[i*train_split:(i+1)*train_split, :], data_train_dead[:, :]) )\n",
      "    label_bal = np.vstack((np.zeros((train_split, 1)), np.ones((n_train_dead, 1))))\n",
      "    \n",
      "    forests[i].fit(data_bal, label_bal)\n",
      "    \n",
      "    print i\n",
      "    C_train.append(metrics.confusion_matrix(label_bal, forests[i].predict(data_bal)))\n",
      "    get_ss(C_train[i])\n",
      "    print\n",
      "    y = forests[i].predict(data_valid)\n",
      "    y_valid[:, i] = y\n",
      "    C_valid.append(metrics.confusion_matrix(labels_valid, y))\n",
      "    get_ss(C_valid[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(y_valid, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 205,
       "text": [
        "array([ 35933.,  35782.,  40528.,  35490.,  35663.,  36448.,  36242.])"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Majority vote?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comp_matrix = np.zeros((n_valid_combined, n_train_splits))\n",
      "for i in xrange(n_train_splits):\n",
      "    comp_matrix[:, i] = y_valid[:, i] == labels_valid[:, 0]\n",
      "print comp_matrix.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(117936, 7)\n"
       ]
      }
     ],
     "prompt_number": 273
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(comp_matrix, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 274,
       "text": [
        "array([ 81163.,  81262.,  77635.,  80204.,  81121.,  80790.,  80836.])"
       ]
      }
     ],
     "prompt_number": 274
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_1 = np.max(comp_matrix, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 307
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_1 = labels_valid.copy()\n",
      "y_1 = 1-y_1;\n",
      "y_1[s_1 == 1] = labels_valid[s_1 == 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 314
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_1.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 322,
       "text": [
        "(117936, 1)"
       ]
      }
     ],
     "prompt_number": 322
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_ss(metrics.confusion_matrix(labels_valid, y_1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[92956  8422]\n",
        " [ 1614 14944]]\n",
        "\n",
        "Sensitiviy:  0.902524459476 \n",
        "Specificity:  0.916924776579\n"
       ]
      }
     ],
     "prompt_number": 323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(np.sum(comp_matrix[labels_valid[:, 0] == 1 ,:], axis=1)>0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 278,
       "text": [
        "14944"
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Using balanced data, but only a part of it\n",
      "f_bal.fit(data_bal, label_bal)\n",
      "print f_bal.oob_score_\n",
      "print f_bal.score(data_bal, label_bal)\n",
      "print f_bal.score(data_valid, labels_valid)\n",
      "C = metrics.confusion_matrix(labels_valid, f_bal.predict(data_valid))\n",
      "get_ss(C)\n",
      "get_ss(metrics.confusion_matrix(label_bal, f_bal.predict(data_bal)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.959974738229\n",
        "0.999844402138"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.722213743047"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[75695 25683]\n",
        " [ 7078  9480]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.914489024199 \n",
        "Specificity:  0.269601569832\n",
        "[[54625     3]\n",
        " [   14 54614]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.999743772763 \n",
        "Specificity:  0.999945072047\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
       ]
      }
     ],
     "prompt_number": 189
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Weigh inversely propotional to sizes\n",
      "w0 = n_dead_train / (n_dead_train + n_living_train)\n",
      "w1 = n_living_train / (n_dead_train + n_living_train)\n",
      "print w0, w1\n",
      "f = RandomForestClassifier(n_estimators=10, max_depth=20, n_jobs=-1, class_weight={0:w0, 1:1} ,oob_score=True)\n",
      "data_train.shape\n",
      "f.fit(data_train, labels_train[:, 0])\n",
      "f.oob_score_\n",
      "f.score(data_valid, labels_valid)\n",
      "sum(labels_valid)\n",
      "metric\n",
      "C = metric.confusion_matrix(labels_train, y_train)\n",
      "y_predicted = f.predict(data_valid)\n",
      "metrics.confusion_matrix(labels_valid, y_predicted)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature importances!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_all = np.vstack((data_train, data_valid))\n",
      "labels_all = np.vstack((labels_train, labels_valid))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forest = RandomForestClassifier(n_estimators=10, n_jobs=-1, max_depth=30)\n",
      "forest.fit(data_all, labels_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 249,
       "text": [
        "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
        "            min_samples_leaf=1, min_samples_split=2,\n",
        "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
        "            oob_score=False, random_state=None, verbose=0,\n",
        "            warm_start=False)"
       ]
      }
     ],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forest = RandomForestClassifier(n_estimators=10, max_depth=30)\n",
      "forest.fit(data_all, labels_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 250,
       "text": [
        "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
        "            min_samples_leaf=1, min_samples_split=2,\n",
        "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0,\n",
        "            warm_start=False)"
       ]
      }
     ],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['indices', 'f', 'std']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "importances = forest.feature_importances_\n",
      "\n",
      "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
      "indices = np.argsort(importances)[::-1]\n",
      "# Print the feature ranking\n",
      "print(\"Feature ranking:\")\n",
      "\n",
      "for f in range(124):\n",
      "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
      "\n",
      "# Plot the feature importances of the forest\n",
      "plt.figure()\n",
      "plt.title(\"Feature importances\")\n",
      "plt.bar(range(124), importances[indices],\n",
      "       color=\"r\", yerr=std[indices], align=\"center\")\n",
      "plt.xticks(range(124), indices)\n",
      "plt.xlim([-1, 125])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Feature ranking:\n",
        "1. feature 3 (0.040234)\n",
        "2. feature 2 (0.038200)\n",
        "3. feature 65 (0.035853)\n",
        "4. feature 4 (0.034280)\n",
        "5. feature 0 (0.030767)\n",
        "6. feature 95 (0.030671)\n",
        "7. feature 1 (0.028306)\n",
        "8. feature 62 (0.028128)\n",
        "9. feature 63 (0.027150)\n",
        "10. feature 64 (0.026501)\n",
        "11. feature 34 (0.026368)\n",
        "12. feature 36 (0.023667)\n",
        "13. feature 33 (0.023517)\n",
        "14. feature 32 (0.023278)\n",
        "15. feature 35 (0.023257)\n",
        "16. feature 96 (0.022870)\n",
        "17. feature 5 (0.022506)\n",
        "18. feature 31 (0.022078)\n",
        "19. feature 93 (0.021525)\n",
        "20. feature 94 (0.020679)\n",
        "21. feature 66 (0.018291)\n",
        "22. feature 98 (0.017011)\n",
        "23. feature 97 (0.016425)\n",
        "24. feature 54 (0.015923)\n",
        "25. feature 67 (0.015543)\n",
        "26. feature 23 (0.015469)\n",
        "27. feature 85 (0.012515)\n",
        "28. feature 12 (0.010980)\n",
        "29. feature 13 (0.010315)\n",
        "30. feature 116 (0.009384)\n",
        "31. feature 9 (0.009249)\n",
        "32. feature 17 (0.008985)\n",
        "33. feature 73 (0.007809)\n",
        "34. feature 40 (0.007608)\n",
        "35. feature 10 (0.007478)\n",
        "36. feature 44 (0.007477)\n",
        "37. feature 41 (0.006922)\n",
        "38. feature 16 (0.006548)\n",
        "39. feature 11 (0.006501)\n",
        "40. feature 26 (0.006352)\n",
        "41. feature 25 (0.006288)\n",
        "42. feature 43 (0.006286)\n",
        "43. feature 102 (0.006131)\n",
        "44. feature 74 (0.005977)\n",
        "45. feature 15 (0.005658)\n",
        "46. feature 48 (0.005343)\n",
        "47. feature 6 (0.005148)\n",
        "48. feature 47 (0.005083)\n",
        "49. feature 20 (0.005066)\n",
        "50. feature 72 (0.005025)\n",
        "51. feature 78 (0.005016)\n",
        "52. feature 14 (0.005001)\n",
        "53. feature 7 (0.004983)\n",
        "54. feature 88 (0.004941)\n",
        "55. feature 42 (0.004938)\n",
        "56. feature 46 (0.004895)\n",
        "57. feature 71 (0.004825)\n",
        "58. feature 56 (0.004758)\n",
        "59. feature 106 (0.004673)\n",
        "60. feature 45 (0.004651)\n",
        "61. feature 77 (0.004575)\n",
        "62. feature 75 (0.004544)\n",
        "63. feature 38 (0.004535)\n",
        "64. feature 39 (0.004427)\n",
        "65. feature 57 (0.004384)\n",
        "66. feature 37 (0.004347)\n",
        "67. feature 69 (0.004286)\n",
        "68. feature 8 (0.004275)\n",
        "69. feature 110 (0.004222)\n",
        "70. feature 103 (0.004101)\n",
        "71. feature 76 (0.004073)\n",
        "72. feature 30 (0.003636)\n",
        "73. feature 70 (0.003632)\n",
        "74. feature 105 (0.003601)\n",
        "75. feature 27 (0.003553)\n",
        "76. feature 61 (0.003444)\n",
        "77. feature 113 (0.003307)\n",
        "78. feature 99 (0.003171)\n",
        "79. feature 118 (0.003070)\n",
        "80. feature 109 (0.002976)\n",
        "81. feature 51 (0.002813)\n",
        "82. feature 87 (0.002745)\n",
        "83. feature 120 (0.002662)\n",
        "84. feature 58 (0.002656)\n",
        "85. feature 28 (0.002647)\n",
        "86. feature 68 (0.002521)\n",
        "87. feature 92 (0.002455)\n",
        "88. feature 107 (0.002452)\n",
        "89. feature 59 (0.002413)\n",
        "90. feature 108 (0.002365)\n",
        "91. feature 123 (0.002341)\n",
        "92. feature 21 (0.002180)\n",
        "93. feature 104 (0.002079)\n",
        "94. feature 83 (0.002013)\n",
        "95. feature 100 (0.001966)\n",
        "96. feature 52 (0.001890)\n",
        "97. feature 101 (0.001716)\n",
        "98. feature 79 (0.001691)\n",
        "99. feature 121 (0.001374)\n",
        "100. feature 119 (0.001370)\n",
        "101. feature 82 (0.001328)\n",
        "102. feature 89 (0.001204)\n",
        "103. feature 90 (0.001079)\n",
        "104. feature 19 (0.000323)\n",
        "105. feature 55 (0.000256)\n",
        "106. feature 50 (0.000246)\n",
        "107. feature 81 (0.000239)\n",
        "108. feature 24 (0.000233)\n",
        "109. feature 114 (0.000223)\n",
        "110. feature 86 (0.000215)\n",
        "111. feature 22 (0.000188)\n",
        "112. feature 115 (0.000134)\n",
        "113. feature 53 (0.000126)\n",
        "114. feature 112 (0.000092)\n",
        "115. feature 60 (0.000080)\n",
        "116. feature 29 (0.000073)\n",
        "117. feature 91 (0.000052)\n",
        "118. feature 117 (0.000039)\n",
        "119. feature 84 (0.000033)\n",
        "120. feature 122 (0.000020)\n",
        "121. feature 111 (0.000003)\n",
        "122. feature 18 (0.000000)\n",
        "123. feature 80 (0.000000)\n",
        "124. feature 49 (0.000000)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f3fc5b581d0>"
       ]
      }
     ],
     "prompt_number": 326
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classify sequences of patients using the mean of the window prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combining the arrays makes predictions much much faster, but adds an overhead to find patients\n",
      "data_train_living = np.vstack(patient_train_living)\n",
      "data_train_dead = np.vstack(patient_train_dead)\n",
      "data_valid_living = np.vstack(patient_valid_living)\n",
      "data_valid_dead = np.vstack(patient_valid_dead)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate Ids of patients in the training and validation separated arrays\n",
      "id_train_living = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_train_living)\n",
      "                             ])\n",
      "id_train_dead = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_train_dead)\n",
      "                             ])\n",
      "id_valid_living = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_valid_living)\n",
      "                             ])\n",
      "id_valid_dead = np.vstack([np.zeros((patient.shape[0], 1)) + i for i, patient in enumerate(patient_valid_dead)\n",
      "                             ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 365
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict separtely for each of the sets\n",
      "pred_train_living = f.predict(data_train_living)\n",
      "pred_train_dead = f.predict(data_train_dead)\n",
      "pred_valid_dead = f.predict(data_valid_dead)\n",
      "pred_valid_living = f.predict(data_valid_living)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 423
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To easily access a specific patient\n",
      "def predp_train_living(i):\n",
      "    return pred_train_living[id_train_living[:, 0] == i]\n",
      "def predp_train_dead(i):\n",
      "    return pred_train_dead[id_train_dead[:, 0] == i]\n",
      "\n",
      "def predp_valid_living(i):\n",
      "    return pred_valid_living[id_valid_living[:, 0] == i]\n",
      "def predp_valid_dead(i):\n",
      "    return pred_valid_dead[id_valid_dead[:, 0] == i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 424
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_valid_living = np.array([np.array(predp_valid_living(i)) for i in xrange(len(patient_valid_living))])\n",
      "y_valid_dead = np.array([np.array(predp_valid_dead(i)) for i in xrange(len(patient_valid_dead))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 438
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_living = np.array([np.array(predp_train_living(i)) for i in xrange(len(patient_train_living))])\n",
      "y_train_dead = np.array([np.array(predp_train_dead(i)) for i in xrange(len(patient_train_dead))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 439
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_valid_living = np.array([np.mean(predp_valid_living(i)) for i in xrange(len(patient_valid_living))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 433
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(y_valid_living < 0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 435,
       "text": [
        "0.9609022556390977"
       ]
      }
     ],
     "prompt_number": 435
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for i in xrange(len(patient_valid_living)):\n",
      "    print np.mean(predp_valid_living(i))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "0.1875\n",
        "0.0229885057471\n",
        "0.433333333333\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0888888888889\n",
        "0.0\n",
        "0.138047138047\n",
        "0.144032921811\n",
        "0.14224137931\n",
        "0.0\n",
        "0.266666666667\n",
        "0.0\n",
        "0.0\n",
        "0.012323943662\n",
        "0.0487804878049\n",
        "0.312072892938\n",
        "0.0\n",
        "0.162303664921\n",
        "0.0\n",
        "0.0\n",
        "0.0760869565217\n",
        "0.0\n",
        "0.0\n",
        "0.148936170213\n",
        "0.0\n",
        "0.0197238658777\n",
        "0.0\n",
        "0.178489702517\n",
        "0.0\n",
        "0.0995260663507\n",
        "0.0120481927711\n",
        "0.0\n",
        "0.0\n",
        "0.00793650793651\n",
        "0.0\n",
        "0.0\n",
        "0.280961182994\n",
        "0.030534351145\n",
        "0.0\n",
        "0.0\n",
        "0.112676056338\n",
        "0.121428571429\n",
        "0.10071942446\n",
        "0.0\n",
        "0.0\n",
        "0.432098765432\n",
        "0.0\n",
        "0.0909090909091\n",
        "0.0\n",
        "0.0932203389831\n",
        "0.0\n",
        "0.070796460177\n",
        "0.0\n",
        "0.441176470588\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00520833333333\n",
        "0.0\n",
        "0.0149253731343\n",
        "0.0\n",
        "0.131661442006\n",
        "0.166666666667\n",
        "0.244956772334\n",
        "0.0280373831776\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.187145557656\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.19298245614\n",
        "0.282402528978\n",
        "0.264462809917\n",
        "0.0247933884298\n",
        "0.048275862069\n",
        "0.0\n",
        "0.0\n",
        "0.328125\n",
        "0.0965732087227\n",
        "0.149377593361\n",
        "0.0267857142857\n",
        "0.0449438202247\n",
        "0.0\n",
        "0.0\n",
        "0.0194174757282\n",
        "0.192307692308\n",
        "0.0142857142857\n",
        "0.0\n",
        "0.0\n",
        "0.0579710144928\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.217948717949\n",
        "0.471074380165\n",
        "0.0\n",
        "0.0724637681159\n",
        "0.0\n",
        "0.227848101266\n",
        "0.012987012987\n",
        "0.244897959184\n",
        "0.0\n",
        "0.0\n",
        "0.0377358490566\n",
        "0.0\n",
        "0.0\n",
        "0.226666666667\n",
        "0.0\n",
        "0.170212765957\n",
        "0.0\n",
        "0.0\n",
        "0.03\n",
        "0.0\n",
        "0.0\n",
        "0.188449848024\n",
        "0.0\n",
        "0.271428571429\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.155172413793\n",
        "0.148648648649\n",
        "0.25\n",
        "0.0\n",
        "0.0\n",
        "0.125\n",
        "0.0526315789474\n",
        "0.0\n",
        "0.00701754385965\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.016393442623\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.190476190476\n",
        "0.194092827004\n",
        "0.0856031128405\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.388838060384\n",
        "0.0\n",
        "0.0195652173913\n",
        "0.0155642023346\n",
        "0.285714285714\n",
        "0.333333333333\n",
        "0.246575342466\n",
        "0.107692307692\n",
        "0.218181818182\n",
        "0.0\n",
        "0.0\n",
        "0.0326086956522\n",
        "0.0\n",
        "0.072864321608\n",
        "0.0\n",
        "0.0191570881226\n",
        "0.129909365559\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0736842105263\n",
        "0.0751252086811\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00461538461538\n",
        "0.00149700598802\n",
        "0.00247524752475\n",
        "0.0\n",
        "0.196335078534\n",
        "0.0567164179104\n",
        "0.0\n",
        "0.0\n",
        "0.166666666667\n",
        "0.178947368421\n",
        "0.390438247012\n",
        "0.00847457627119\n",
        "0.0\n",
        "0.207207207207\n",
        "0.0980392156863\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.16393442623\n",
        "0.0352760736196\n",
        "0.0\n",
        "0.0333333333333\n",
        "0.0\n",
        "0.036036036036\n",
        "0.0637362637363\n",
        "0.0\n",
        "0.0970873786408\n",
        "0.0\n",
        "0.0361445783133\n",
        "0.05\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.121951219512\n",
        "0.0\n",
        "0.0\n",
        "0.197674418605\n",
        "0.0155440414508\n",
        "0.0\n",
        "0.0120192307692\n",
        "0.0309734513274\n",
        "0.0823529411765\n",
        "0.0\n",
        "0.0181818181818\n",
        "0.0703125\n",
        "0.0\n",
        "0.162629757785\n",
        "0.0\n",
        "0.0\n",
        "0.0121951219512\n",
        "0.00970873786408\n",
        "0.0583333333333\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.116438356164\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0645161290323\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.122302158273\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0505836575875\n",
        "0.0173913043478\n",
        "0.109022556391\n",
        "0.133333333333\n",
        "0.0\n",
        "0.115384615385\n",
        "0.0\n",
        "0.0334728033473\n",
        "0.277777777778\n",
        "0.00819672131148\n",
        "0.0484848484848\n",
        "0.00595238095238\n",
        "0.12\n",
        "0.0\n",
        "0.222921914358\n",
        "0.0\n",
        "0.0\n",
        "0.0188679245283\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0208333333333\n",
        "0.0890410958904\n",
        "0.0196078431373\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.105734767025\n",
        "0.265402843602\n",
        "0.0\n",
        "0.416666666667\n",
        "0.0740740740741\n",
        "0.0\n",
        "0.0902255639098\n",
        "0.0\n",
        "0.0\n",
        "0.103448275862\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.359154929577\n",
        "0.0\n",
        "0.0\n",
        "0.06\n",
        "0.0552995391705\n",
        "0.0\n",
        "0.379562043796\n",
        "0.165876777251\n",
        "0.256013745704\n",
        "0.0521172638436\n",
        "0.0\n",
        "0.182186234818\n",
        "0.262295081967\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0294117647059\n",
        "0.0\n",
        "0.364341085271\n",
        "0.133333333333\n",
        "0.0\n",
        "0.149028077754\n",
        "0.0\n",
        "0.172413793103\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0125\n",
        "0.211267605634\n",
        "0.0496688741722\n",
        "0.0149253731343\n",
        "0.0173010380623\n",
        "0.0\n",
        "0.300429184549\n",
        "0.0\n",
        "0.0\n",
        "0.212543554007\n",
        "0.0375\n",
        "0.0\n",
        "0.0839694656489\n",
        "0.0\n",
        "0.00136612021858\n",
        "0.00970873786408\n",
        "0.352813852814\n",
        "0.073732718894\n",
        "0.04\n",
        "0.0188679245283\n",
        "0.0\n",
        "0.117647058824\n",
        "0.0\n",
        "0.2\n",
        "0.0736196319018\n",
        "0.0\n",
        "0.0210970464135\n",
        "0.0434782608696\n",
        "0.0\n",
        "0.223880597015\n",
        "0.00363636363636\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.225\n",
        "0.0\n",
        "0.0\n",
        "0.10752688172\n",
        "0.0\n",
        "0.0\n",
        "0.0377358490566\n",
        "0.0555555555556\n",
        "0.015625\n",
        "0.0384615384615\n",
        "0.16835016835\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.181818181818\n",
        "0.0\n",
        "0.0\n",
        "0.089715536105\n",
        "0.025\n",
        "0.0\n",
        "0.00540540540541\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.11320754717\n",
        "0.0\n",
        "0.0\n",
        "0.0617283950617\n",
        "0.397402597403\n",
        "0.005\n",
        "0.0229885057471\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0696864111498\n",
        "0.0\n",
        "0.00411522633745\n",
        "0.265306122449\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.303825956489\n",
        "0.0384615384615\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0319634703196\n",
        "0.0344827586207\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.5\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00434782608696\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0277777777778\n",
        "0.192592592593\n",
        "0.0\n",
        "0.0778947368421\n",
        "0.0204081632653\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.00772200772201\n",
        "0.0\n",
        "0.127272727273\n",
        "0.392572944297\n",
        "0.043795620438\n",
        "0.0\n",
        "0.0\n",
        "0.00819672131148\n",
        "0.0406504065041\n",
        "0.0125\n",
        "0.0\n",
        "0.142857142857\n",
        "0.0\n",
        "0.093023255814\n",
        "0.0\n",
        "0.0\n",
        "0.110655737705\n",
        "0.00220750551876\n",
        "0.0\n",
        "0.0137457044674\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.145695364238\n",
        "0.0111111111111\n",
        "0.0\n",
        "0.0606060606061\n",
        "0.0172413793103\n",
        "0.00540540540541\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.136801541426\n",
        "0.0\n",
        "0.0\n",
        "0.16814159292\n",
        "0.0520487264673\n",
        "0.0\n",
        "0.0551181102362\n",
        "0.135135135135\n",
        "0.183098591549\n",
        "0.205128205128\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0845070422535\n",
        "0.018691588785\n",
        "0.0227272727273\n",
        "0.0901639344262\n",
        "0.0\n",
        "0.0176470588235\n",
        "0.0108695652174\n",
        "0.0\n",
        "0.185897435897\n",
        "0.0\n",
        "0.22972972973\n",
        "0.217391304348\n",
        "0.0570469798658\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.197580645161\n",
        "0.224880382775\n",
        "0.0666666666667\n",
        "0.0178571428571\n",
        "0.0952380952381\n",
        "0.0491803278689\n",
        "0.138888888889\n",
        "0.386666666667\n",
        "0.0\n",
        "0.120728929385\n",
        "0.0223880597015\n",
        "0.0\n",
        "0.0\n",
        "0.0432432432432\n",
        "0.0170454545455\n",
        "0.122270742358\n",
        "0.0\n",
        "0.336956521739\n",
        "0.0062893081761\n",
        "0.0\n",
        "0.0\n",
        "0.131578947368\n",
        "0.393939393939\n",
        "0.0\n",
        "0.0224215246637\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.046875\n",
        "0.0\n",
        "0.158536585366\n",
        "0.0\n",
        "0.0\n",
        "0.025069637883\n",
        "0.191044776119\n",
        "0.025641025641\n",
        "0.158878504673\n",
        "0.0\n",
        "0.025\n",
        "0.025974025974\n",
        "0.0384615384615\n",
        "0.013698630137\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.182978723404\n",
        "0.0\n",
        "0.0919540229885\n",
        "0.0609318996416\n",
        "0.122935779817\n",
        "0.0\n",
        "0.0666666666667\n",
        "0.00369003690037\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.204819277108\n",
        "0.0327868852459\n",
        "0.15\n",
        "0.0240384615385\n",
        "0.047619047619\n",
        "0.0\n",
        "0.016393442623\n",
        "0.00540540540541\n",
        "0.0\n",
        "0.0\n",
        "0.0818181818182\n",
        "0.0\n",
        "0.245901639344\n",
        "0.046511627907\n",
        "0.269911504425\n",
        "0.0\n",
        "0.0217391304348\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.118644067797\n",
        "0.0\n",
        "0.0\n",
        "0.731707317073\n",
        "0.310160427807\n",
        "0.120689655172\n",
        "0.0\n",
        "0.0268096514745\n",
        "0.00881057268722\n",
        "0.0\n",
        "0.0\n",
        "0.270358306189\n",
        "0.0\n",
        "0.152173913043\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.089527027027\n",
        "0.0\n",
        "0.156862745098\n",
        "0.0\n",
        "0.0277777777778\n",
        "0.0324675324675\n",
        "0.252032520325\n",
        "0.0140845070423\n",
        "0.0736842105263\n",
        "0.648648648649\n",
        "0.0\n",
        "0.0\n",
        "0.283636363636\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0177304964539\n",
        "0.288025889968\n",
        "0.00719424460432\n",
        "0.0\n",
        "0.0325581395349\n",
        "0.0243902439024\n",
        "0.0078431372549\n",
        "0.0\n",
        "0.0710227272727\n",
        "0.0\n",
        "0.0\n",
        "0.193103448276\n",
        "0.230182926829\n",
        "0.0\n",
        "0.0\n",
        "0.0425531914894\n",
        "0.0\n",
        "0.281176470588\n",
        "0.0128755364807\n",
        "0.388190954774\n",
        "0.0994623655914\n",
        "0.0368098159509\n",
        "0.0\n",
        "0.0\n",
        "0.0234899328859\n",
        "0.0\n",
        "0.0\n",
        "0.0384615384615\n",
        "0.0\n",
        "0.0555555555556\n",
        "0.0\n",
        "0.0\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 413
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RF reports :P\n",
      "- Works well (a little too well) in classifying the outputs\n",
      "\n",
      "## TODO\n",
      "- use the rf to classify the windows of the other time series\n",
      "- Try using mean again\n",
      "\n",
      "### SKEW SKEW SKEW!!\n",
      "- \n",
      "- Use the classification of the RF on the windows as some feature for an hmm, or other classifier?\n",
      "    - The idea is that we want the model to get surer that it has seen some consistent 1 windows before it starts throwing out 1s.\n",
      "- Age, ICU!\n",
      "\n",
      "###  Done\n",
      "- impute missing values with -1 for RF\n",
      "- get patient wise data, split into 2 sets\n",
      "- generate matrices using 1, and train RF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}