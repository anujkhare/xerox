{
 "metadata": {
  "name": "",
  "signature": "sha256:8851e1b9cec214c075c1511c2217696c5b3ba4cf284c9394971500cf778d6e6d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import pandas\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from pandas import Series, DataFrame\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import svm\n",
      "from sklearn.metrics import confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import *\n",
      "from scipy.stats import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 368
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_age = pd.read_csv('data_train/id_age_train.csv')\n",
      "df_labels = pd.read_csv('data_train/id_label_train.csv')\n",
      "df_vitals = pd.read_csv('data_train/id_time_vitals_train.csv')\n",
      "df_labs = pd.read_csv('data_train/id_time_labs_train.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Normalize following the procedure in the paper\n",
      "\n",
      "#### Find the xL, xU, w1, w2, w3 vars"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_combined = df_vitals.drop('ICU', axis=1).copy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_combined = df_combined.join(df_labs.drop(['ID', 'TIME'], axis=1))\n",
      "df_combined.drop(['ID', 'TIME'], axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features = 31"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_range = np.zeros((n_features, 2))\n",
      "W = np.zeros((n_features, 3))\n",
      "for i in xrange(n_features):\n",
      "#for i in xrange(1):\n",
      "    vals = df_combined.ix[df_combined.ix[:, i].notnull(), i]\n",
      "    vals = np.sort(vals)\n",
      "\n",
      "    # find the quantiles\n",
      "    N = vals.shape[0]\n",
      "    q = np.array([(j-0.5)/N for j in xrange(N)])\n",
      "    \n",
      "    i_l = np.min(np.arange(q.shape[0])[q>0.01])\n",
      "    i_u = np.max(np.arange(q.shape[0])[q<0.99])\n",
      "    \n",
      "    x_range[i, 0] = vals[i_l]\n",
      "    x_range[i, 1] = vals[i_u]\n",
      "\n",
      "    vals = vals[i_l: i_u+1]\n",
      "    q=norm.ppf(q[i_l: i_u+1]) / 3\n",
      "\n",
      "    X = np.ones((vals.shape[0], 3))\n",
      "    X[:, 1] = vals\n",
      "    X[:, 2] = np.log(1 + vals)\n",
      "\n",
      "    W[i, :] = np.dot(np.linalg.pinv(np.dot(X.T, X)),\n",
      "                     np.dot(X.T, q)\n",
      "                     )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print x_range[0], W[0], norm_transform(100, x_range[0], W[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "array([  77.,  184.])"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Process the entire data using this process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# transform one element given range and W\n",
      "def norm_transform(x, x_range, W):\n",
      "    if(x<x_range[0]):\n",
      "        x=x_range[0]\n",
      "    if(x>x_range[1]):\n",
      "        x=x_range[1]\n",
      "        \n",
      "    return np.dot(W,\n",
      "                  [1, x, log(1+x)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm = df_combined.copy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(n_features):\n",
      "    df_norm.ix[:, i] = df_norm.ix[:, i].apply(lambda x:norm_transform(x, x_range[i], W[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm[df_norm.isnull()] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>V1</th>\n",
        "      <th>V2</th>\n",
        "      <th>V3</th>\n",
        "      <th>V4</th>\n",
        "      <th>V5</th>\n",
        "      <th>V6</th>\n",
        "      <th>L1</th>\n",
        "      <th>L2</th>\n",
        "      <th>L3</th>\n",
        "      <th>L4</th>\n",
        "      <th>...</th>\n",
        "      <th>L16</th>\n",
        "      <th>L17</th>\n",
        "      <th>L18</th>\n",
        "      <th>L19</th>\n",
        "      <th>L20</th>\n",
        "      <th>L21</th>\n",
        "      <th>L22</th>\n",
        "      <th>L23</th>\n",
        "      <th>L24</th>\n",
        "      <th>L25</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 423137.000000</td>\n",
        "      <td> 423038.000000</td>\n",
        "      <td> 455161.000000</td>\n",
        "      <td> 430491.000000</td>\n",
        "      <td> 435631.000000</td>\n",
        "      <td> 157943.000000</td>\n",
        "      <td> 12471.000000</td>\n",
        "      <td> 12790.000000</td>\n",
        "      <td> 12831.000000</td>\n",
        "      <td> 32719.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 4709.000000</td>\n",
        "      <td> 104.000000</td>\n",
        "      <td> 106199.000000</td>\n",
        "      <td> 1484.000000</td>\n",
        "      <td> 6330.000000</td>\n",
        "      <td> 13184.000000</td>\n",
        "      <td> 10879.000000</td>\n",
        "      <td> 5775.000000</td>\n",
        "      <td> 459.000000</td>\n",
        "      <td> 16990.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>     -0.000802</td>\n",
        "      <td>     -0.000518</td>\n",
        "      <td>      0.000193</td>\n",
        "      <td>      0.000262</td>\n",
        "      <td>     -0.003010</td>\n",
        "      <td>     -0.001436</td>\n",
        "      <td>     0.001861</td>\n",
        "      <td>    -0.001999</td>\n",
        "      <td>    -0.001785</td>\n",
        "      <td>    -0.000847</td>\n",
        "      <td>...</td>\n",
        "      <td>   -0.000726</td>\n",
        "      <td>  -0.004222</td>\n",
        "      <td>     -0.001889</td>\n",
        "      <td>   -0.002667</td>\n",
        "      <td>   -0.003260</td>\n",
        "      <td>    -0.000022</td>\n",
        "      <td>    -0.001577</td>\n",
        "      <td>   -0.001157</td>\n",
        "      <td>  -0.004562</td>\n",
        "      <td>    -0.000263</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>      0.327613</td>\n",
        "      <td>      0.328750</td>\n",
        "      <td>      0.326887</td>\n",
        "      <td>      0.328365</td>\n",
        "      <td>      0.310592</td>\n",
        "      <td>      0.335399</td>\n",
        "      <td>     0.326107</td>\n",
        "      <td>     0.332202</td>\n",
        "      <td>     0.323635</td>\n",
        "      <td>     0.338183</td>\n",
        "      <td>...</td>\n",
        "      <td>    0.297590</td>\n",
        "      <td>   0.265866</td>\n",
        "      <td>      0.329994</td>\n",
        "      <td>    0.319238</td>\n",
        "      <td>    0.307946</td>\n",
        "      <td>     0.326704</td>\n",
        "      <td>     0.324033</td>\n",
        "      <td>    0.325643</td>\n",
        "      <td>   0.334086</td>\n",
        "      <td>     0.331819</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>     -0.823122</td>\n",
        "      <td>     -0.832645</td>\n",
        "      <td>     -0.757585</td>\n",
        "      <td>     -0.814802</td>\n",
        "      <td>     -0.693755</td>\n",
        "      <td>     -1.039356</td>\n",
        "      <td>    -0.699635</td>\n",
        "      <td>    -0.986676</td>\n",
        "      <td>    -0.803858</td>\n",
        "      <td>    -1.063083</td>\n",
        "      <td>...</td>\n",
        "      <td>   -0.378839</td>\n",
        "      <td>  -0.219519</td>\n",
        "      <td>     -0.926135</td>\n",
        "      <td>   -0.722979</td>\n",
        "      <td>   -0.801997</td>\n",
        "      <td>    -0.758326</td>\n",
        "      <td>    -0.783260</td>\n",
        "      <td>   -0.800160</td>\n",
        "      <td>  -0.946023</td>\n",
        "      <td>    -0.888026</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>     -0.232650</td>\n",
        "      <td>     -0.215903</td>\n",
        "      <td>     -0.234004</td>\n",
        "      <td>     -0.178394</td>\n",
        "      <td>     -0.242745</td>\n",
        "      <td>     -0.188750</td>\n",
        "      <td>    -0.220671</td>\n",
        "      <td>    -0.192651</td>\n",
        "      <td>    -0.187963</td>\n",
        "      <td>    -0.166365</td>\n",
        "      <td>...</td>\n",
        "      <td>   -0.224691</td>\n",
        "      <td>  -0.180796</td>\n",
        "      <td>     -0.216556</td>\n",
        "      <td>   -0.240028</td>\n",
        "      <td>   -0.177055</td>\n",
        "      <td>    -0.228905</td>\n",
        "      <td>    -0.223231</td>\n",
        "      <td>   -0.199659</td>\n",
        "      <td>  -0.195983</td>\n",
        "      <td>    -0.217783</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>     -0.007559</td>\n",
        "      <td>     -0.001985</td>\n",
        "      <td>     -0.003235</td>\n",
        "      <td>      0.019892</td>\n",
        "      <td>     -0.018887</td>\n",
        "      <td>      0.002622</td>\n",
        "      <td>     0.012262</td>\n",
        "      <td>    -0.005978</td>\n",
        "      <td>    -0.019656</td>\n",
        "      <td>    -0.008838</td>\n",
        "      <td>...</td>\n",
        "      <td>   -0.038657</td>\n",
        "      <td>  -0.121243</td>\n",
        "      <td>     -0.005452</td>\n",
        "      <td>   -0.033856</td>\n",
        "      <td>   -0.084112</td>\n",
        "      <td>     0.001487</td>\n",
        "      <td>    -0.017543</td>\n",
        "      <td>    0.017752</td>\n",
        "      <td>  -0.019236</td>\n",
        "      <td>    -0.007321</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>      0.229670</td>\n",
        "      <td>      0.213289</td>\n",
        "      <td>      0.218119</td>\n",
        "      <td>      0.191199</td>\n",
        "      <td>      0.245840</td>\n",
        "      <td>      0.181659</td>\n",
        "      <td>     0.223407</td>\n",
        "      <td>     0.195469</td>\n",
        "      <td>     0.200614</td>\n",
        "      <td>     0.184304</td>\n",
        "      <td>...</td>\n",
        "      <td>    0.101068</td>\n",
        "      <td>   0.029716</td>\n",
        "      <td>      0.217963</td>\n",
        "      <td>    0.225265</td>\n",
        "      <td>    0.244771</td>\n",
        "      <td>     0.221762</td>\n",
        "      <td>     0.216481</td>\n",
        "      <td>    0.202394</td>\n",
        "      <td>   0.197205</td>\n",
        "      <td>     0.217760</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>      0.743244</td>\n",
        "      <td>      0.781180</td>\n",
        "      <td>      0.777265</td>\n",
        "      <td>      0.841406</td>\n",
        "      <td>      0.393024</td>\n",
        "      <td>      0.896911</td>\n",
        "      <td>     0.847445</td>\n",
        "      <td>     0.800909</td>\n",
        "      <td>     0.636214</td>\n",
        "      <td>     0.984638</td>\n",
        "      <td>...</td>\n",
        "      <td>    0.723557</td>\n",
        "      <td>   0.758355</td>\n",
        "      <td>      0.738847</td>\n",
        "      <td>    0.642935</td>\n",
        "      <td>    0.495006</td>\n",
        "      <td>     0.767727</td>\n",
        "      <td>     0.659725</td>\n",
        "      <td>    0.731318</td>\n",
        "      <td>   0.895594</td>\n",
        "      <td>     0.872130</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 31 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "                  V1             V2             V3             V4  \\\n",
        "count  423137.000000  423038.000000  455161.000000  430491.000000   \n",
        "mean       -0.000802      -0.000518       0.000193       0.000262   \n",
        "std         0.327613       0.328750       0.326887       0.328365   \n",
        "min        -0.823122      -0.832645      -0.757585      -0.814802   \n",
        "25%        -0.232650      -0.215903      -0.234004      -0.178394   \n",
        "50%        -0.007559      -0.001985      -0.003235       0.019892   \n",
        "75%         0.229670       0.213289       0.218119       0.191199   \n",
        "max         0.743244       0.781180       0.777265       0.841406   \n",
        "\n",
        "                  V5             V6            L1            L2            L3  \\\n",
        "count  435631.000000  157943.000000  12471.000000  12790.000000  12831.000000   \n",
        "mean       -0.003010      -0.001436      0.001861     -0.001999     -0.001785   \n",
        "std         0.310592       0.335399      0.326107      0.332202      0.323635   \n",
        "min        -0.693755      -1.039356     -0.699635     -0.986676     -0.803858   \n",
        "25%        -0.242745      -0.188750     -0.220671     -0.192651     -0.187963   \n",
        "50%        -0.018887       0.002622      0.012262     -0.005978     -0.019656   \n",
        "75%         0.245840       0.181659      0.223407      0.195469      0.200614   \n",
        "max         0.393024       0.896911      0.847445      0.800909      0.636214   \n",
        "\n",
        "                 L4      ...               L16         L17            L18  \\\n",
        "count  32719.000000      ...       4709.000000  104.000000  106199.000000   \n",
        "mean      -0.000847      ...         -0.000726   -0.004222      -0.001889   \n",
        "std        0.338183      ...          0.297590    0.265866       0.329994   \n",
        "min       -1.063083      ...         -0.378839   -0.219519      -0.926135   \n",
        "25%       -0.166365      ...         -0.224691   -0.180796      -0.216556   \n",
        "50%       -0.008838      ...         -0.038657   -0.121243      -0.005452   \n",
        "75%        0.184304      ...          0.101068    0.029716       0.217963   \n",
        "max        0.984638      ...          0.723557    0.758355       0.738847   \n",
        "\n",
        "               L19          L20           L21           L22          L23  \\\n",
        "count  1484.000000  6330.000000  13184.000000  10879.000000  5775.000000   \n",
        "mean     -0.002667    -0.003260     -0.000022     -0.001577    -0.001157   \n",
        "std       0.319238     0.307946      0.326704      0.324033     0.325643   \n",
        "min      -0.722979    -0.801997     -0.758326     -0.783260    -0.800160   \n",
        "25%      -0.240028    -0.177055     -0.228905     -0.223231    -0.199659   \n",
        "50%      -0.033856    -0.084112      0.001487     -0.017543     0.017752   \n",
        "75%       0.225265     0.244771      0.221762      0.216481     0.202394   \n",
        "max       0.642935     0.495006      0.767727      0.659725     0.731318   \n",
        "\n",
        "              L24           L25  \n",
        "count  459.000000  16990.000000  \n",
        "mean    -0.004562     -0.000263  \n",
        "std      0.334086      0.331819  \n",
        "min     -0.946023     -0.888026  \n",
        "25%     -0.195983     -0.217783  \n",
        "50%     -0.019236     -0.007321  \n",
        "75%      0.197205      0.217760  \n",
        "max      0.895594      0.872130  \n",
        "\n",
        "[8 rows x 31 columns]"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(df_norm.V5.values, bins=50);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fbfa34678d0>"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df_norm\n",
      "\n",
      "patient_combined = [np.array(df.ix[df_vitals['ID'] == ID, :]) for ID in df_labels['ID'].unique()]\n",
      "\n",
      "# split into living and dead\n",
      "patient_combined_dead = [patient_combined[ID-1] for ID in df_labels['ID']\n",
      "                                                      if df_labels.ix[ID - 1, 'LABEL'] == 1]\n",
      "patient_combined_living = [patient_combined[ID-1] for ID in df_labels['ID']\n",
      "                                                      if df_labels.ix[ID - 1, 'LABEL'] == 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patient_combined_living[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "(1956, 31)"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features = 31\n",
      "n_stats = 4\n",
      "n_dead = len(patient_combined_dead)\n",
      "n_living = len(patient_combined_living)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_windows = 3\n",
      "stats = [np.nanmean, np.nanmin, np.nanmax]\n",
      "#stats = [np.nanmean, np.nanmin, np.nanmax, lambda x:x.shape[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stats[3]()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 140,
       "text": [
        "9"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_n_windows(ts, n_windows, stats):\n",
      "    n_features = ts.shape[1]\n",
      "    l = np.zeros((n_windows, len(stats) * n_features))\n",
      "    \n",
      "    n = int(ts.shape[0] / n_windows)\n",
      "    for j in xrange(0, n_windows):\n",
      "        if (j==n_windows-1):\n",
      "            window = ts[j*n :, :] \n",
      "        else:\n",
      "            window = ts[j*n : (j+1)*n, :] \n",
      "    \n",
      "        for k in xrange(len(stats)):\n",
      "            f = stats[k]\n",
      "            l[j, k*n_features:(k+1)*n_features] = f(window, axis=0)\n",
      "    return l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_windows_data(data, n_windows, stats):\n",
      "    window_stats = []\n",
      "    for i in xrange(len(data)):\n",
      "        ts = data[i]\n",
      "\n",
      "        if(ts.shape[0] < n_windows):\n",
      "            #window_stats.append(None)    # We want to keep 1-1 mapping with patients..\n",
      "            continue\n",
      "\n",
      "        l = get_n_windows(ts, n_windows, stats)\n",
      "        window_stats.append(l)\n",
      "        \n",
      "    return window_stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_rows_data(data, n_windows, stats):\n",
      "    data_windows = np.zeros((len(data), n_windows*data[0].shape[1]*len(stats)))\n",
      "    for i in xrange(len(data)):\n",
      "        ts = data[i]\n",
      "\n",
      "        if(ts.shape[0] < n_windows):\n",
      "            #window_stats.append(None)    # We want to keep 1-1 mapping with patients..\n",
      "            continue\n",
      "\n",
      "        l = get_n_windows(ts, n_windows, stats)\n",
      "        data_windows[i, :] = np.hstack(l)\n",
      "        \n",
      "    return data_windows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Confusion!\n",
      "def get_ss(confusion):\n",
      "    tn = confusion[0, 0]  # label=0, pred=0\n",
      "    fp = confusion[0, 1]  # label=0, pred=1\n",
      "    \n",
      "    fn = confusion[1, 0]  # label=1, pred=0\n",
      "    tp = confusion[1, 1]  # label=1, pred=1\n",
      "    \n",
      "    sens = (1.0*tp/(tp+fn))\n",
      "    spec = (1.0*tn/(tn+fp))\n",
      "    \n",
      "    print confusion\n",
      "    print\n",
      "    #print tp, fp, fn, tn, sens, spec\n",
      "    print 'Sensitiviy: ', sens, '\\nSpecificity: ', spec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 289
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Split Data!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_train_valid(data_combined, labels_combined, split=0.8):\n",
      "    \n",
      "    ind = np.arange(0, len(labels_combined))\n",
      "    np.random.shuffle(ind)\n",
      "    \n",
      "    N = len(ind)\n",
      "    N_train = int(split * N)\n",
      "    \n",
      "    ind_train = ind[:N_train]\n",
      "    ind_valid = ind[N_train:]\n",
      "    data_train = data_combined[ind_train]\n",
      "    labels_train = labels_combined[ind_train]\n",
      "\n",
      "    data_valid = data_combined[ind_valid]\n",
      "    labels_valid = labels_combined[ind_valid]\n",
      "    \n",
      "    return data_train, labels_train, data_valid, labels_valid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 310
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Single forest on all the data\n",
      "forest = RandomForestClassifier(n_estimators=10000, max_depth=None, oob_score=False, n_jobs=-1)\n",
      "forest.fit(data_train, labels_train)\n",
      "get_ss(confusion_matrix(labels_valid, forest.predict(data_valid)))\n",
      "get_ss(confusion_matrix(labels_train, forest.predict(data_train)))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Separate dead and alive in train and validation sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def separate_living_dead(data_train, labels_train, data_valid, labels_valid):\n",
      "    data_train_living = data_train[labels_train == 0]\n",
      "    data_train_dead = data_train[labels_train == 1]\n",
      "    data_valid_living = data_valid[labels_valid == 0]\n",
      "    data_valid_dead = data_valid[labels_valid == 1]\n",
      "    \n",
      "    return data_train_living, data_train_dead, data_valid_living, data_valid_dead"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 339
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Find number of dead and alive patients in train and validation sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sizes(data_train_living, data_train_dead, data_valid_living, data_valid_dead):\n",
      "    n_train_dead = data_train_dead.shape[0]\n",
      "    n_train_living = data_train_living.shape[0]\n",
      "    n_train_combined = n_train_dead + n_train_living\n",
      "    n_valid_dead = data_valid_dead.shape[0]\n",
      "    n_valid_living = data_valid_living.shape[0]\n",
      "    n_valid_combined = n_valid_living + n_valid_dead\n",
      "    \n",
      "    return n_train_dead, n_train_living, n_train_combined, n_valid_dead, n_valid_living, n_valid_combined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 348
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generate windowed data\n",
      "data_windows_combined = get_rows_data(patient_combined, 2, stats)\n",
      "labels_combined = np.array(df_labels.LABEL)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### split into train and validation sets, living and dead\n",
      "data_train, labels_train, data_valid, labels_valid = split_train_valid(data_windows_combined, labels_combined, split=0.9)\n",
      "data_train_living, data_train_dead, data_valid_living, data_valid_dead = separate_living_dead(data_train, labels_train,\n",
      "                                                                                              data_valid, labels_valid)\n",
      "[n_train_dead, n_train_living, n_train_combined, n_valid_dead,\n",
      "                     n_valid_living, n_valid_combined] = get_sizes(data_train_living, data_train_dead, \n",
      "                                                                   data_valid_living, data_valid_dead)\n",
      "print n_train_dead, n_train_living, labels_train.shape[0]\n",
      "print n_valid_dead, n_valid_living, labels_valid.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "217 3017 3234\n",
        "28 332 360\n"
       ]
      }
     ],
     "prompt_number": 457
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Split into balacned sets!\n",
      "n_train_splits = int (n_train_living / n_train_dead)\n",
      "train_split = int (n_train_living / n_train_splits)\n",
      "print n_train_splits, train_split, n_train_dead + n_train_splits * train_split, n_train_combined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 538
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classify!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Train balanced models!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_balanced(models):\n",
      "    C_train = []\n",
      "    C_valid = []\n",
      "    #y_valid = np.zeros((n_valid_combined, n_train_splits))\n",
      "    for i in xrange(n_train_splits):\n",
      "        # NO RANDOM SHUFFLE NOW!\n",
      "        data_bal = np.vstack((data_train_living[i * train_split: (i+1) * train_split, :],\n",
      "                              data_train_dead[:, :]) )\n",
      "        \n",
      "        label_bal = np.vstack((np.zeros((train_split, 1)),\n",
      "                               np.ones((n_train_dead, 1))))\n",
      "\n",
      "        models[i].fit(data_bal, label_bal[:, 0])\n",
      "\n",
      "        C_train.append(confusion_matrix(label_bal[:,], models[i].predict(data_bal)))\n",
      "        y = models[i].predict(data_valid)\n",
      "        C_valid.append(confusion_matrix(labels_valid[:], y))\n",
      "        get_ss(C_valid[i])\n",
      "        \n",
      "    return C_train, C_valid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 500
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_proba(models_bal, data):\n",
      "    # Returns negative log probability of dying\n",
      "    dead_probs = np.array([-model.predict_log_proba(data)[:, 1] for model in models_bal]).T\n",
      "    return dead_probs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 525
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_glm(models_bal, glm):\n",
      "   # find the probability estimates of each of the balanced models on the training set \n",
      "    \n",
      "    dead_probs_train = get_proba(models_bal, data_train)\n",
      "    glm.fit(dead_probs_train, labels_train)\n",
      "    \n",
      "    print 'Training data:'\n",
      "    get_ss(confusion_matrix(labels_train, glm.predict(dead_probs_train)))\n",
      "    print 'Validation data:'\n",
      "    get_ss(confusion_matrix(labels_valid, glm.predict(get_proba(models_bal, data_valid))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 532
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svms = [svm.NuSVC(kernel='poly', degree=2, gamma=10**(-1.7), probability=True) for i in xrange(n_train_splits)]\n",
      "forests = [RandomForestClassifier(n_estimators=40, n_jobs=-1, max_depth=40) for i in xrange(n_train_splits)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 484
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "glm = [LogisticRegression(), LogisticRegressionCV(), SGDClassifier()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 540
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c_train, c_valid = train_balanced(svms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[312  20]\n",
        " [  5  23]]\n",
        "\n",
        "Sensitiviy:  0.821428571429 \n",
        "Specificity:  0.939759036145\n",
        "[[310  22]\n",
        " [  5  23]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.821428571429 \n",
        "Specificity:  0.933734939759\n",
        "[[310  22]\n",
        " [  6  22]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.785714285714 \n",
        "Specificity:  0.933734939759\n",
        "[[315  17]\n",
        " [  7  21]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.75 \n",
        "Specificity:  0.948795180723\n",
        "[[314  18]\n",
        " [  8  20]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.714285714286 \n",
        "Specificity:  0.94578313253\n",
        "[[316  16]\n",
        " [  6  22]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.785714285714 \n",
        "Specificity:  0.951807228916\n",
        "[[314  18]\n",
        " [  6  22]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.785714285714 \n",
        "Specificity:  0.94578313253\n",
        "[[314  18]\n",
        " [  4  24]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.857142857143 \n",
        "Specificity:  0.94578313253\n",
        "[[310  22]\n",
        " [  6  22]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.785714285714 \n",
        "Specificity:  0.933734939759\n",
        "[[316  16]\n",
        " [  7  21]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.75 \n",
        "Specificity:  0.951807228916\n",
        "[[315  17]\n",
        " [  7  21]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.75 \n",
        "Specificity:  0.948795180723\n",
        "[[314  18]\n",
        " [  7  21]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.75 \n",
        "Specificity:  0.94578313253\n",
        "[[310  22]\n",
        " [  6  22]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.785714285714 \n",
        "Specificity:  0.933734939759\n"
       ]
      }
     ],
     "prompt_number": 533
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_glm(svms, glm[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Training data:\n",
        "[[2956   61]\n",
        " [  72  145]]\n",
        "\n",
        "Sensitiviy:  0.668202764977 \n",
        "Specificity:  0.979781239642\n",
        "Validation data:\n",
        "[[323   9]\n",
        " [ 10  18]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.642857142857 \n",
        "Specificity:  0.972891566265\n"
       ]
      }
     ],
     "prompt_number": 531
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_glm(svms, glm[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training data:\n",
        "[[2953   64]\n",
        " [  64  153]]\n",
        "\n",
        "Sensitiviy:  0.705069124424 \n",
        "Specificity:  0.978786874379\n",
        "Validation data:\n",
        "[[322  10]\n",
        " [ 10  18]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.642857142857 \n",
        "Specificity:  0.969879518072\n"
       ]
      }
     ],
     "prompt_number": 537
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_glm(svms, glm[2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training data:\n",
        "[[2953   64]\n",
        " [  71  146]]\n",
        "\n",
        "Sensitiviy:  0.672811059908 \n",
        "Specificity:  0.978786874379\n",
        "Validation data:\n",
        "[[324   8]\n",
        " [ 12  16]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.571428571429 \n",
        "Specificity:  0.975903614458\n"
       ]
      }
     ],
     "prompt_number": 541
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_ss(c_train[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[227   5]\n",
        " [ 47 170]]\n",
        "\n",
        "Sensitiviy:  0.783410138249 \n",
        "Specificity:  0.978448275862\n"
       ]
      }
     ],
     "prompt_number": 535
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_balanced(forests);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[609  69]\n",
        " [ 10  31]]\n",
        "\n",
        "Sensitiviy:  0.756097560976 \n",
        "Specificity:  0.898230088496\n",
        "[[616  62]\n",
        " [ 10  31]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.756097560976 \n",
        "Specificity:  0.908554572271\n",
        "[[587  91]\n",
        " [  7  34]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.829268292683 \n",
        "Specificity:  0.865781710914\n",
        "[[616  62]\n",
        " [  7  34]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.829268292683 \n",
        "Specificity:  0.908554572271\n",
        "[[590  88]\n",
        " [  7  34]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.829268292683 \n",
        "Specificity:  0.870206489676\n",
        "[[598  80]\n",
        " [  8  33]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.80487804878 \n",
        "Specificity:  0.882005899705\n",
        "[[606  72]\n",
        " [ 10  31]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.756097560976 \n",
        "Specificity:  0.893805309735\n",
        "[[607  71]\n",
        " [  8  33]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.80487804878 \n",
        "Specificity:  0.895280235988\n",
        "[[613  65]\n",
        " [  7  34]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.829268292683 \n",
        "Specificity:  0.90412979351\n",
        "[[610  68]\n",
        " [  6  35]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.853658536585 \n",
        "Specificity:  0.899705014749\n",
        "[[593  85]\n",
        " [  7  34]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.829268292683 \n",
        "Specificity:  0.874631268437\n",
        "[[588  90]\n",
        " [  6  35]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.853658536585 \n",
        "Specificity:  0.867256637168\n",
        "[[624  54]\n",
        " [  5  36]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Sensitiviy:  0.878048780488 \n",
        "Specificity:  0.920353982301\n"
       ]
      }
     ],
     "prompt_number": 301
    }
   ],
   "metadata": {}
  }
 ]
}